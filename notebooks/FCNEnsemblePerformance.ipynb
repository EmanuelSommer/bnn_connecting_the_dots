{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute performance for non-Bayesian deep ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T05:38:34.350418231Z",
     "start_time": "2024-02-01T05:38:29.942571385Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Final, List\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import sys\n",
    "from numpyro.distributions import Normal\n",
    "sys.path.append('../')\n",
    "from experiments.fcn_bnns.utils.analysis_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T05:38:36.263484638Z",
     "start_time": "2024-02-01T05:38:36.209334754Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_sizes: List[int],\n",
    "        activation: nn.modules.activation,\n",
    "        dropout_ratio: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Instantiate MLP.\"\"\"\n",
    "        super().__init__()\n",
    "        hidden_id = '_'.join([str(x) for x in hidden_sizes])\n",
    "        self.model_id = f'MLP_{input_size}_{hidden_id}_2'\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.net = torch.nn.Sequential(torch.nn.Linear(input_size, hidden_sizes[0]))\n",
    "        for i, o in zip(hidden_sizes, hidden_sizes[1:] + [2]):\n",
    "            self.net.append(activation())\n",
    "            self.net.append(torch.nn.Linear(i, o))\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Define forward pass.\"\"\"\n",
    "        x = self.net(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T05:38:37.777177153Z",
     "start_time": "2024-02-01T05:38:37.743538972Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_rmse(y_true, y_pred):\n",
    "    return torch.sqrt(torch.mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T05:53:20.166196793Z",
     "start_time": "2024-02-01T05:53:20.113774288Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASETS: Final = [\"airfoil\", \"bikesharing\", \"concrete\", \"energy\", \"yacht\", \"protein\"]\n",
    "ARCHITECTURES: Final = [\"16-16\"]\n",
    "ACTIVATIONS: Final = [\"relu\"]\n",
    "REPLICATIONS: Final = [1]  # [1, 2, 3]\n",
    "BATCH_SIZE: Final = [32, 64]  # [32, 64, -1] \n",
    "WEIGHT_DECAY: Final = [0.01, 0.001]  # [0.01, 0.001, 0.0001]\n",
    "VAL_SIZE: Final = [0.1]\n",
    "ENSEMBLE_SIZE: Final = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T05:48:37.711803411Z",
     "start_time": "2024-02-01T05:48:37.705967726Z"
    }
   },
   "outputs": [],
   "source": [
    "main_dir = '../results/'\n",
    "os.makedirs(os.path.join(main_dir, 'de_perf'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T05:48:40.543783488Z",
     "start_time": "2024-02-01T05:48:40.540651462Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_lppd_de(y_true, mean_pred, std_pred):\n",
    "    \"\"\"variant of lppd for de samples\"\"\"\n",
    "    # log_prob_means = []\n",
    "    # for idx in range(y_true.shape[0]):\n",
    "    #     yi = y_true[idx]\n",
    "    #     mean = mean_pred[:, idx].mean(0)  # ensemble mean\n",
    "    #     std_in = jnp.exp(std_pred[:, idx])  # individual stds\n",
    "    #     # variance = (jnp.power(std_in, 2) + jnp.power(mean_pred[:, idx], 2)).mean(0) - jnp.power(mean, 2)\n",
    "    #     variance = (std_in + jnp.power(mean_pred[:, idx], 2)).mean(0) - jnp.power(mean, 2)\n",
    "    #     std = jnp.power(variance, 0.5)\n",
    "    #     predictive_prob = Normal(mean, std).log_prob(yi)\n",
    "    #     log_prob_means.append(predictive_prob)\n",
    "    # log_prob_means = jnp.array(log_prob_means)\n",
    "    # return log_prob_means[np.isfinite(log_prob_means)]\n",
    "    log_prob_means = []\n",
    "    for idx in range(y_true.shape[0]):\n",
    "        yi = y_true[idx]\n",
    "        log_prob_means_m = []\n",
    "        for m in range(mean_pred.shape[0]):\n",
    "            mean = mean_pred[m, idx]\n",
    "            std_in = jnp.power(jnp.exp(std_pred[m, idx]), 0.5)\n",
    "            predictive_prob = jnp.exp(Normal(mean, std_in).log_prob(yi))\n",
    "            log_prob_means_m.append(predictive_prob)\n",
    "        log_prob_means.append(jnp.array(log_prob_means_m).mean(0))\n",
    "    log_prob_means = jnp.log(jnp.array(log_prob_means))\n",
    "    return log_prob_means[np.isfinite(log_prob_means)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "rows_grid_search = []\n",
    "for ds, wd, bs in itertools.product(DATASETS, WEIGHT_DECAY, BATCH_SIZE):\n",
    "    identifier = [ds, wd, bs]\n",
    "    dirname = f'{ds}.data|16-16|tanh|wd{str(wd)}|bs{str(bs)}|val|1|'\n",
    "    exp_info = {\"data\": f'{ds}.data', \"replications\": 1}\n",
    "    regr_dataset = pml.data.dataset.DatasetTabular(\n",
    "        data_path=f'../data/{ds}.data',\n",
    "        target_indices=[],\n",
    "        split_spec={'train': 0.7, 'val': 0.1, 'test': 0.2},\n",
    "        seed=1,\n",
    "        standardize=True,\n",
    "    )\n",
    "    X_test, Y_test = regr_dataset.get_data(split='test', data_type='jax')\n",
    "    if ds in [\"bikesharing\", \"protein\"]:\n",
    "        X_test = X_test[:2000, :]\n",
    "        Y_test = Y_test[:2000, :]\n",
    "    X_test = torch.from_numpy(np.array(X_test))\n",
    "    Y_test = torch.from_numpy(np.array(Y_test)).squeeze()\n",
    "    ensemble_mean = []\n",
    "    ensemble_sd = []\n",
    "    for i in range(ENSEMBLE_SIZE):\n",
    "        weight_dict = torch.load(os.path.join(main_dir, 'de', f\"{dirname}/stdict_{i}.pt\"))\n",
    "        model = MLP(\n",
    "            input_size=X_test.shape[1], \n",
    "            hidden_sizes=[16, 16], \n",
    "            activation=nn.Tanh, \n",
    "            dropout_ratio=0.\n",
    "        )\n",
    "        model.load_state_dict(weight_dict)\n",
    "        outputs = model(X_test)\n",
    "        ensemble_mean.append(outputs[:, 0])\n",
    "        ensemble_sd.append(outputs[:, 1])\n",
    "    ensemble_mean_agg = torch.stack(tuple(ensemble_mean)).detach()\n",
    "    ensemble_sd_agg = torch.stack(tuple(ensemble_sd)).detach()\n",
    "    rmse_ensemble = compute_rmse(Y_test, ensemble_mean_agg.mean(0)).numpy()\n",
    "    lppd_ensemble = compute_lppd_de(Y_test.numpy(), ensemble_mean_agg.numpy(), ensemble_sd_agg.numpy())\n",
    "    rows_grid_search.append(identifier + [rmse_ensemble])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T06:47:07.917523636Z",
     "start_time": "2024-02-01T06:44:52.732239601Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "        dataset  weight_decay  batch_size    rmse\n2       airfoil         0.001          32  0.2853\n3       airfoil         0.001          64  0.2858\n0       airfoil         0.010          32  0.2951\n1       airfoil         0.010          64  0.3183\n6   bikesharing         0.001          32  0.2832\n7   bikesharing         0.001          64  0.2886\n4   bikesharing         0.010          32  0.3289\n5   bikesharing         0.010          64  0.3392\n10     concrete         0.001          32  0.3566\n11     concrete         0.001          64  0.3652\n8      concrete         0.010          32  0.3592\n9      concrete         0.010          64  0.3609\n14       energy         0.001          32  0.2156\n15       energy         0.001          64  0.2134\n12       energy         0.010          32  0.2123\n13       energy         0.010          64  0.2165\n22      protein         0.001          32  0.7280\n23      protein         0.001          64  0.7281\n20      protein         0.010          32  0.7943\n21      protein         0.010          64  0.7937\n18        yacht         0.001          32  0.6188\n19        yacht         0.001          64  0.5277\n16        yacht         0.010          32  0.5355\n17        yacht         0.010          64  0.6066",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>weight_decay</th>\n      <th>batch_size</th>\n      <th>rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>airfoil</td>\n      <td>0.001</td>\n      <td>32</td>\n      <td>0.2853</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>airfoil</td>\n      <td>0.001</td>\n      <td>64</td>\n      <td>0.2858</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>airfoil</td>\n      <td>0.010</td>\n      <td>32</td>\n      <td>0.2951</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>airfoil</td>\n      <td>0.010</td>\n      <td>64</td>\n      <td>0.3183</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bikesharing</td>\n      <td>0.001</td>\n      <td>32</td>\n      <td>0.2832</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>bikesharing</td>\n      <td>0.001</td>\n      <td>64</td>\n      <td>0.2886</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bikesharing</td>\n      <td>0.010</td>\n      <td>32</td>\n      <td>0.3289</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>bikesharing</td>\n      <td>0.010</td>\n      <td>64</td>\n      <td>0.3392</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>concrete</td>\n      <td>0.001</td>\n      <td>32</td>\n      <td>0.3566</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>concrete</td>\n      <td>0.001</td>\n      <td>64</td>\n      <td>0.3652</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>concrete</td>\n      <td>0.010</td>\n      <td>32</td>\n      <td>0.3592</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>concrete</td>\n      <td>0.010</td>\n      <td>64</td>\n      <td>0.3609</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>energy</td>\n      <td>0.001</td>\n      <td>32</td>\n      <td>0.2156</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>energy</td>\n      <td>0.001</td>\n      <td>64</td>\n      <td>0.2134</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>energy</td>\n      <td>0.010</td>\n      <td>32</td>\n      <td>0.2123</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>energy</td>\n      <td>0.010</td>\n      <td>64</td>\n      <td>0.2165</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>protein</td>\n      <td>0.001</td>\n      <td>32</td>\n      <td>0.7280</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>protein</td>\n      <td>0.001</td>\n      <td>64</td>\n      <td>0.7281</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>protein</td>\n      <td>0.010</td>\n      <td>32</td>\n      <td>0.7943</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>protein</td>\n      <td>0.010</td>\n      <td>64</td>\n      <td>0.7937</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>yacht</td>\n      <td>0.001</td>\n      <td>32</td>\n      <td>0.6188</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>yacht</td>\n      <td>0.001</td>\n      <td>64</td>\n      <td>0.5277</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>yacht</td>\n      <td>0.010</td>\n      <td>32</td>\n      <td>0.5355</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>yacht</td>\n      <td>0.010</td>\n      <td>64</td>\n      <td>0.6066</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    rows_grid_search, columns=['dataset', 'weight_decay', 'batch_size', 'rmse']\n",
    ")\n",
    "df.sort_values(['dataset', 'weight_decay', 'batch_size'], inplace=True)\n",
    "df['rmse'] = df['rmse'].apply(lambda x: f'{x:.4f}')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T06:51:58.939078338Z",
     "start_time": "2024-02-01T06:51:58.935132495Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrl}\n",
      "\\toprule\n",
      "dataset & weight_decay & batch_size & rmse \\\\\n",
      "\\midrule\n",
      "airfoil & 0.0010 & 32 & 0.2853 \\\\\n",
      "airfoil & 0.0010 & 64 & 0.2858 \\\\\n",
      "airfoil & 0.0100 & 32 & 0.2951 \\\\\n",
      "airfoil & 0.0100 & 64 & 0.3183 \\\\\n",
      "bikesharing & 0.0010 & 32 & 0.2832 \\\\\n",
      "bikesharing & 0.0010 & 64 & 0.2886 \\\\\n",
      "bikesharing & 0.0100 & 32 & 0.3289 \\\\\n",
      "bikesharing & 0.0100 & 64 & 0.3392 \\\\\n",
      "concrete & 0.0010 & 32 & 0.3566 \\\\\n",
      "concrete & 0.0010 & 64 & 0.3652 \\\\\n",
      "concrete & 0.0100 & 32 & 0.3592 \\\\\n",
      "concrete & 0.0100 & 64 & 0.3609 \\\\\n",
      "energy & 0.0010 & 32 & 0.2156 \\\\\n",
      "energy & 0.0010 & 64 & 0.2134 \\\\\n",
      "energy & 0.0100 & 32 & 0.2123 \\\\\n",
      "energy & 0.0100 & 64 & 0.2165 \\\\\n",
      "protein & 0.0010 & 32 & 0.7280 \\\\\n",
      "protein & 0.0010 & 64 & 0.7281 \\\\\n",
      "protein & 0.0100 & 32 & 0.7943 \\\\\n",
      "protein & 0.0100 & 64 & 0.7937 \\\\\n",
      "yacht & 0.0010 & 32 & 0.6188 \\\\\n",
      "yacht & 0.0010 & 64 & 0.5277 \\\\\n",
      "yacht & 0.0100 & 32 & 0.5355 \\\\\n",
      "yacht & 0.0100 & 64 & 0.6066 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    df.to_latex(\n",
    "        index=False, \n",
    "        formatters={\"name\": str.upper}, \n",
    "        float_format=\"{:.4f}\".format,\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T06:51:14.392605133Z",
     "start_time": "2024-02-01T06:51:14.378014235Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(os.path.join(main_dir, 'de_perf'), 'de_perf_grid_search.csv'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance for ReLU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T00:01:38.819725145Z",
     "start_time": "2024-01-30T23:54:27.954819181Z"
    }
   },
   "outputs": [],
   "source": [
    "rows_bnn_relu = []\n",
    "for ds, rep in itertools.product(DATASETS, REPLICATIONS):\n",
    "    identifier = [ds, rep]\n",
    "    dirname = f'{ds}.data|16-16|relu|{str(rep)}|'\n",
    "    exp_info = {\"data\": f'{ds}.data', \"replications\": rep}\n",
    "    regr_dataset = pml.data.dataset.DatasetTabular(\n",
    "        data_path=f'../data/{ds}.data',\n",
    "        target_indices=[],\n",
    "        split_spec={'train': 0.8, 'test': 0.2},\n",
    "        seed=rep,\n",
    "        standardize=True,\n",
    "    )\n",
    "    X_test, Y_test = regr_dataset.get_data(split='test', data_type='jax')\n",
    "    if ds in [\"bikesharing\", \"protein\"]:\n",
    "        X_test = X_test[:2000, :]\n",
    "        Y_test = Y_test[:2000, :]\n",
    "    X_test = torch.from_numpy(np.array(X_test))\n",
    "    Y_test = torch.from_numpy(np.array(Y_test)).squeeze()\n",
    "    ensemble_mean = []\n",
    "    ensemble_sd = []\n",
    "    for i in range(ENSEMBLE_SIZE):\n",
    "        weight_dict = torch.load(os.path.join(main_dir, 'de', f\"{dirname}/stdict_{i}.pt\"))\n",
    "        model = MLP(\n",
    "            input_size=X_test.shape[1], \n",
    "            hidden_sizes=[16, 16], \n",
    "            activation=nn.ReLU, \n",
    "            dropout_ratio=0.\n",
    "        )\n",
    "        model.load_state_dict(weight_dict)\n",
    "        outputs = model(X_test)\n",
    "        ensemble_mean.append(outputs[:, 0])\n",
    "        ensemble_sd.append(outputs[:, 1])\n",
    "    ensemble_mean_agg = torch.stack(tuple(ensemble_mean)).detach()\n",
    "    ensemble_sd_agg = torch.stack(tuple(ensemble_sd)).detach()\n",
    "    rmse_ensemble = compute_rmse(Y_test, ensemble_mean_agg.mean(0)).numpy()\n",
    "    rmse_individual = [compute_rmse(Y_test, ensemble_mean_agg[i]).numpy() for i in range(ensemble_mean_agg.shape[0])]\n",
    "    lppd_ensemble = compute_lppd_de(Y_test.numpy(), ensemble_mean_agg.numpy(), ensemble_sd_agg.numpy())\n",
    "    # lppd_ensemble_finite = lppd_ensemble[np.isfinite(lppd_ensemble)].mean(0)\n",
    "    lppd_individual = [\n",
    "        compute_lppd_de(Y_test.numpy(), ensemble_mean_agg[i].unsqueeze(0).numpy(), ensemble_sd_agg[i].unsqueeze(0).numpy()) \n",
    "        for i in range(ensemble_mean_agg.shape[0])\n",
    "    ]\n",
    "    lppd_individual = [lppd_individual[i].mean(0) for i in range(len(lppd_individual))]\n",
    "    rmse_individual_avg = sum(rmse_individual) / len(rmse_individual)\n",
    "    lppd_individual_avg = sum(lppd_individual) / len(lppd_individual)\n",
    "    rows_bnn_relu.append(identifier + [rmse_ensemble, rmse_individual_avg, lppd_ensemble.mean(0), lppd_individual_avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T00:10:08.658196169Z",
     "start_time": "2024-01-31T00:10:08.594914699Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    rows_bnn_relu, columns=['dataset', 'rep', 'rmse_ensemble', 'rmse_ind', 'lppd_ensemble', 'lppd_ind']\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T00:10:27.352240780Z",
     "start_time": "2024-01-31T00:10:27.340235756Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(os.path.join(main_dir, 'de_perf_playground'), 'aggregated_data_de.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
