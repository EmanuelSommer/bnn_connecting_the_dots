{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute performance for non-Bayesian deep ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Final, List\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import sys\n",
    "from numpyro.distributions import Normal\n",
    "sys.path.append('../')\n",
    "from experiments.fcn_bnns.utils.analysis_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS: Final = [\"airfoil\", \"bikesharing\", \"concrete\", \"energy\", \"yacht\", \"protein\"]\n",
    "ARCHITECTURES: Final = [\"16-16\"]\n",
    "ACTIVATIONS: Final = [\"relu\"]\n",
    "REPLICATIONS: Final = [1, 2, 3]\n",
    "BATCH_SIZE: Final = [32, 64, -1] \n",
    "WEIGHT_DECAY: Final = [0.01, 0.001, 0.0001]\n",
    "VAL_SIZE: Final = [0.1]\n",
    "ENSEMBLE_SIZE: Final = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_sizes: List[int],\n",
    "        activation: nn.modules.activation,\n",
    "        dropout_ratio: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Instantiate MLP.\"\"\"\n",
    "        super().__init__()\n",
    "        hidden_id = '_'.join([str(x) for x in hidden_sizes])\n",
    "        self.model_id = f'MLP_{input_size}_{hidden_id}_2'\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.net = torch.nn.Sequential(torch.nn.Linear(input_size, hidden_sizes[0]))\n",
    "        for i, o in zip(hidden_sizes, hidden_sizes[1:] + [2]):\n",
    "            self.net.append(activation())\n",
    "            self.net.append(torch.nn.Linear(i, o))\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Define forward pass.\"\"\"\n",
    "        x = self.net(x)\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_rmse(y_true, y_pred):\n",
    "    return torch.sqrt(torch.mean((y_true - y_pred)**2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lppd_de(y_true, mean_pred, std_pred):\n",
    "    \"\"\"variant of lppd for de samples\"\"\"\n",
    "    log_prob_means = []\n",
    "    for idx in range(y_true.shape[0]):\n",
    "        yi = y_true[idx]\n",
    "        log_prob_means_m = []\n",
    "        for m in range(mean_pred.shape[0]):\n",
    "            mean = mean_pred[m, idx]\n",
    "            std_in = jnp.power(jnp.exp(std_pred[m, idx]), 0.5)\n",
    "            predictive_prob = jnp.exp(Normal(mean, std_in).log_prob(yi))\n",
    "            log_prob_means_m.append(predictive_prob)\n",
    "        log_prob_means.append(jnp.array(log_prob_means_m).mean(0))\n",
    "    log_prob_means = jnp.log(jnp.array(log_prob_means))\n",
    "    return log_prob_means[np.isfinite(log_prob_means)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main_dir = '../../results/'\n",
    "os.makedirs(os.path.join(main_dir, 'de_perf'), exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rows_grid_search = []\n",
    "for ds, wd, bs in itertools.product(DATASETS, WEIGHT_DECAY, BATCH_SIZE):\n",
    "    identifier = [ds, wd, bs]\n",
    "    dirname = f'{ds}.data|16-16|tanh|wd{str(wd)}|bs{str(bs)}|val|1|'\n",
    "    exp_info = {\"data\": f'{ds}.data', \"replications\": 1}\n",
    "    regr_dataset = pml.data.dataset.DatasetTabular(\n",
    "        data_path=f'../data/{ds}.data',\n",
    "        target_indices=[],\n",
    "        split_spec={'train': 0.7, 'val': 0.1, 'test': 0.2},\n",
    "        seed=1,\n",
    "        standardize=True,\n",
    "    )\n",
    "    X_test, Y_test = regr_dataset.get_data(split='test', data_type='jax')\n",
    "    if ds in [\"bikesharing\", \"protein\"]:\n",
    "        X_test = X_test[:2000, :]\n",
    "        Y_test = Y_test[:2000, :]\n",
    "    X_test = torch.from_numpy(np.array(X_test))\n",
    "    Y_test = torch.from_numpy(np.array(Y_test)).squeeze()\n",
    "    ensemble_mean = []\n",
    "    ensemble_sd = []\n",
    "    for i in range(ENSEMBLE_SIZE):\n",
    "        weight_dict = torch.load(os.path.join(main_dir, 'de', f\"{dirname}/stdict_{i}.pt\"))\n",
    "        model = MLP(\n",
    "            input_size=X_test.shape[1], \n",
    "            hidden_sizes=[16, 16], \n",
    "            activation=nn.Tanh, \n",
    "            dropout_ratio=0.\n",
    "        )\n",
    "        model.load_state_dict(weight_dict)\n",
    "        outputs = model(X_test)\n",
    "        ensemble_mean.append(outputs[:, 0])\n",
    "        ensemble_sd.append(outputs[:, 1])\n",
    "    ensemble_mean_agg = torch.stack(tuple(ensemble_mean)).detach()\n",
    "    ensemble_sd_agg = torch.stack(tuple(ensemble_sd)).detach()\n",
    "    rmse_ensemble = compute_rmse(Y_test, ensemble_mean_agg.mean(0)).numpy()\n",
    "    lppd_ensemble = compute_lppd_de(Y_test.numpy(), ensemble_mean_agg.numpy(), ensemble_sd_agg.numpy())\n",
    "    rows_grid_search.append(identifier + [rmse_ensemble])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    rows_grid_search, columns=['dataset', 'weight_decay', 'batch_size', 'rmse']\n",
    ")\n",
    "df.sort_values(['dataset', 'weight_decay', 'batch_size'], inplace=True)\n",
    "df['rmse'] = df['rmse'].apply(lambda x: f'{x:.4f}')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance for ReLU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_bnn_relu = []\n",
    "for ds, rep in itertools.product(DATASETS, REPLICATIONS):\n",
    "    identifier = [ds, rep]\n",
    "    dirname = f'{ds}.data|16-16|relu|{str(rep)}|'\n",
    "    exp_info = {\"data\": f'{ds}.data', \"replications\": rep}\n",
    "    regr_dataset = pml.data.dataset.DatasetTabular(\n",
    "        data_path=f'../data/{ds}.data',\n",
    "        target_indices=[],\n",
    "        split_spec={'train': 0.8, 'test': 0.2},\n",
    "        seed=rep,\n",
    "        standardize=True,\n",
    "    )\n",
    "    X_test, Y_test = regr_dataset.get_data(split='test', data_type='jax')\n",
    "    if ds in [\"bikesharing\", \"protein\"]:\n",
    "        X_test = X_test[:2000, :]\n",
    "        Y_test = Y_test[:2000, :]\n",
    "    X_test = torch.from_numpy(np.array(X_test))\n",
    "    Y_test = torch.from_numpy(np.array(Y_test)).squeeze()\n",
    "    ensemble_mean = []\n",
    "    ensemble_sd = []\n",
    "    for i in range(ENSEMBLE_SIZE):\n",
    "        weight_dict = torch.load(os.path.join(main_dir, 'de', f\"{dirname}/stdict_{i}.pt\"))\n",
    "        model = MLP(\n",
    "            input_size=X_test.shape[1], \n",
    "            hidden_sizes=[16, 16], \n",
    "            activation=nn.ReLU, \n",
    "            dropout_ratio=0.\n",
    "        )\n",
    "        model.load_state_dict(weight_dict)\n",
    "        outputs = model(X_test)\n",
    "        ensemble_mean.append(outputs[:, 0])\n",
    "        ensemble_sd.append(outputs[:, 1])\n",
    "    ensemble_mean_agg = torch.stack(tuple(ensemble_mean)).detach()\n",
    "    ensemble_sd_agg = torch.stack(tuple(ensemble_sd)).detach()\n",
    "    rmse_ensemble = compute_rmse(Y_test, ensemble_mean_agg.mean(0)).numpy()\n",
    "    rmse_individual = [compute_rmse(Y_test, ensemble_mean_agg[i]).numpy() for i in range(ensemble_mean_agg.shape[0])]\n",
    "    lppd_ensemble = compute_lppd_de(Y_test.numpy(), ensemble_mean_agg.numpy(), ensemble_sd_agg.numpy())\n",
    "    lppd_individual = [\n",
    "        compute_lppd_de(Y_test.numpy(), ensemble_mean_agg[i].unsqueeze(0).numpy(), ensemble_sd_agg[i].unsqueeze(0).numpy()) \n",
    "        for i in range(ensemble_mean_agg.shape[0])\n",
    "    ]\n",
    "    lppd_individual = [lppd_individual[i].mean(0) for i in range(len(lppd_individual))]\n",
    "    rmse_individual_avg = sum(rmse_individual) / len(rmse_individual)\n",
    "    lppd_individual_avg = sum(lppd_individual) / len(lppd_individual)\n",
    "    rows_bnn_relu.append(identifier + [rmse_ensemble, rmse_individual_avg, lppd_ensemble.mean(0), lppd_individual_avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    rows_bnn_relu, columns=['dataset', 'rep', 'rmse_ensemble', 'rmse_ind', 'lppd_ensemble', 'lppd_ind']\n",
    ")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
