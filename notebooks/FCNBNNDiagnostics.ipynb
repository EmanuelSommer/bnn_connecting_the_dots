{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the posterior samples and their induced models from selected experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from IPython.display import display\n",
    "import jax.numpy as jnp\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import pandas as pd\n",
    "from numpyro.diagnostics import hpdi\n",
    "sys.path.append('../')\n",
    "from experiments.fcn_bnns.utils.analysis_utils import *\n",
    "from src.utils import (  # noqa: E402\n",
    "    mse,\n",
    ")\n",
    "from src.visualization.posterior_predictive import (  # noqa: E402\n",
    "    pp_interchain_means,\n",
    "    visualize_pp_chain_means,\n",
    ")\n",
    "from src.utils import flatten_chain_dimension\n",
    "from experiments.fcn_bnns.utils.ui_utils import (  # noqa: E402\n",
    "    calculate_diagnostics,\n",
    "    plot_sample_paths,\n",
    "    visualize_ess,\n",
    "    visualize_pp_rhat,\n",
    "    visualize_rhat,\n",
    ")\n",
    "from src.diagnostics.gelman import split_chain_r_hat, gelman_split_r_hat  # noqa: E402\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datet = \"YYYY-MM-DD-00-00-00\"\n",
    "CONFIG_PATH = f'../results/fcn_bnns/{datet}/config.yaml'\n",
    "DATA_PATH = '../data'\n",
    "replication = 1\n",
    "exp_names = get_exp_names(path=f\"../results/fcn_bnns/{datet}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'airfoil'\n",
    "exp_name = f'{DATASET}.data|tanh|2|2|100|False|NUTS_tiny|{replication}|1|Normal'\n",
    "exp_name = [ename for ename in exp_names if exp_name in ename][0]\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_info = extract_exp_info(exp_name)\n",
    "config = load_config(CONFIG_PATH)\n",
    "n_chains = int(exp_info['n_chains'])\n",
    "n_samples = int(exp_info['n_samples'])\n",
    "X_train, Y_train = load_data(exp_info, splittype='train', data_path=DATA_PATH)\n",
    "X_val, Y_val = load_data(exp_info, splittype='val', data_path=DATA_PATH)\n",
    "val_threshold = min(1000, X_val.shape[0])\n",
    "X_val = X_val[:val_threshold, :]\n",
    "Y_val = Y_val[:val_threshold, :]\n",
    "linear_regr, rf_regr = fit_baselines(X_train, Y_train)\n",
    "mse_linear, mse_rf = evaluate_baselines(X_val, Y_val, linear_regr, rf_regr)\n",
    "res_dict = {}\n",
    "res_dict['rmse_linear'] = np.sqrt(mse_linear)\n",
    "res_dict['rmse_rf'] = np.sqrt(mse_rf)\n",
    "discard_warmup = 10000 if config[\"keep_warmup\"] else 0\n",
    "posterior_samples, posterior_samples_raw = load_samples(exp_name, f'../results/fcn_bnns/{datet}', discard_warmup=discard_warmup)\n",
    "model = load_model(exp_name, f'../results/fcn_bnns/{datet}')\n",
    "preds_chain_dim, preds = get_posterior_predictive(\n",
    "    model, posterior_samples_raw, X_val, exp_info['n_chains']\n",
    ")\n",
    "rmse_per_chain = {}\n",
    "for i in range(preds_chain_dim.shape[0]):\n",
    "    rmse_per_chain[f'chain_{i}'] = np.sqrt(mse(preds_chain_dim[i], Y_val)[0])\n",
    "rmse_table = pd.DataFrame(rmse_per_chain, index=['RMSE']).T\n",
    "bad_chains = rmse_table[rmse_table['RMSE'] > np.sqrt(mse_linear)].index\n",
    "bad_chains = bad_chains.str.split('_').str[1].astype(int).values\n",
    "bad_chains = bad_chains.tolist()\n",
    "good_chains = [i for i in range(n_chains) if i not in bad_chains]\n",
    "if len(good_chains) > 0:\n",
    "    good_chains_pred_indices = np.concatenate(\n",
    "        [np.arange(n_samples) + (n_samples * i) for i in good_chains]\n",
    "    )\n",
    "    good_chains_pred_indices_100 = np.concatenate(\n",
    "        [np.arange(100) + (n_samples * i) for i in good_chains]\n",
    "    )\n",
    "res_dict['n_bad_chains'] = len(bad_chains)\n",
    "res_dict['n_good_chains'] = len(good_chains)\n",
    "if len(good_chains) == 0:\n",
    "    res_dict['rmse_good_chains'] = np.nan\n",
    "    res_dict['rmse_good_chains_100'] = np.nan\n",
    "    res_dict['acc_90hpdi'] = np.nan\n",
    "    res_dict['acc_90hpdi_100'] = np.nan\n",
    "else:\n",
    "    # RMSE\n",
    "    res_dict['rmse_good_chains'] = np.sqrt(\n",
    "        mse(preds[good_chains_pred_indices, :], Y_val)[0]\n",
    "    )\n",
    "    res_dict['rmse_good_chains_100'] = np.sqrt(\n",
    "        mse(preds[good_chains_pred_indices_100, :], Y_val)[0]\n",
    "    )\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_samples = config[\"n_samples\"]\n",
    "# truncate_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_per_chain = {}\n",
    "for i in range(preds_chain_dim.shape[0]):\n",
    "    rmse_per_chain[f'chain_{i}'] = np.sqrt(mse(preds_chain_dim[i], Y_val)[0])\n",
    "rmse_table = pd.DataFrame(rmse_per_chain, index=['RMSE']).T\n",
    "bad_chains = rmse_table[rmse_table['RMSE'] > res_dict[\"rmse_linear\"]].index\n",
    "bad_chains = bad_chains.str.split('_').str[1].astype(int).values\n",
    "bad_chains = bad_chains.tolist()\n",
    "good_chains = [i for i in range(n_chains) if i not in bad_chains]\n",
    "n_samples = config['n_samples']\n",
    "good_chains_pred_indices = np.concatenate(\n",
    "    [np.arange(truncate_samples) + (n_samples * i) for i in good_chains]\n",
    ")\n",
    "rmse_table = rmse_table.sort_values(by='RMSE', ascending=True)\n",
    "# all entries of the df that are > np.sqrt(mse_linear_model) should get a red\n",
    "# background\n",
    "def color_cells(x):\n",
    "    \"\"\"Color the cells of the table.\"\"\"\n",
    "    return 'background-color: red' if x > res_dict[\"rmse_linear\"] else ''\n",
    "rmse_table = rmse_table.style.map(color_cells)\n",
    "rmse_table = rmse_table.format('{:.3f}')\n",
    "rmse_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze performance grid over samples and chains\n",
    "\n",
    "Also save the data for better plotting in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all 1,2,3,..,10, 50, 100, 500 and every other 500 until truncate_samples\n",
    "sample_steps = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "sample_steps += [50, 100]\n",
    "sample_steps += list(range(500, truncate_samples, 500))\n",
    "sample_steps += [truncate_samples]\n",
    "sample_steps = np.unique(sample_steps)\n",
    "sample_steps = sample_steps[sample_steps <= truncate_samples]\n",
    "sample_steps = sample_steps.tolist()\n",
    "# calculate the rmse for each combination of samples and chains\n",
    "rmse_over_samples_and_chains = []\n",
    "for n_samples in sample_steps:\n",
    "    for i, n_chains in enumerate(good_chains):\n",
    "        rmse_over_samples_and_chains.append(\n",
    "            np.sqrt(\n",
    "                mse(\n",
    "                    preds_chain_dim[good_chains[: i + 1], :n_samples, ...].reshape(\n",
    "                        -1, *preds_chain_dim.shape[2:]\n",
    "                    ),\n",
    "                    Y_val,\n",
    "                )[0]\n",
    "            )\n",
    "        )\n",
    "# visualize the rmse over samples and chains using a heatmap\n",
    "rmse_over_samples_and_chains = np.array(rmse_over_samples_and_chains).reshape(\n",
    "    len(sample_steps), len(good_chains)\n",
    ")\n",
    "rmse_over_samples_and_chains = rmse_over_samples_and_chains[::-1, :]\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    rmse_over_samples_and_chains,\n",
    "    xticklabels=[c for c in range(1, len(good_chains)+1)],\n",
    "    yticklabels=sample_steps[::-1],\n",
    "    cmap='YlGn_r',\n",
    ")\n",
    "# find the indices of the minimum value in the heatmap\n",
    "min_idx = np.unravel_index(\n",
    "    np.nanargmin(rmse_over_samples_and_chains),\n",
    "    rmse_over_samples_and_chains.shape,\n",
    ")\n",
    "print(min_idx)\n",
    "# annotate with a red cross\n",
    "plt.scatter(min_idx[1]+0.5, min_idx[0]+0.5, marker='x', color='white')\n",
    "plt.xlabel('Number of Chains')\n",
    "plt.ylabel('Number of Samples (Non-Linear!)')\n",
    "plt.title(\n",
    "    (\n",
    "        'RMSE over Samples and'\n",
    "        ' Chains (Lower is better)'\n",
    "    )\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the heatmap\n",
    "rmse_over_samples_and_chains_df = pd.DataFrame(\n",
    "    rmse_over_samples_and_chains,\n",
    "    index=sample_steps[::-1],\n",
    "    columns=good_chains,\n",
    ")\n",
    "rmse_over_samples_and_chains_df.to_csv(\n",
    "    f'../paper_bde/practical_sbi/chains_samples_grid/{DATASET}_rmse_over_samples_and_chains.csv'\n",
    ")\n",
    "rmse_over_samples_and_chains_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lppd_pointwise = model.get_lppd(X_val, Y_val, posterior_samples_raw, rolling=False)\n",
    "lppd_pointwise_chain_dim = add_chain_dimension({'lppd': lppd_pointwise}, n_chains=exp_info[\"n_chains\"])['lppd']\n",
    "ppd_pointwise_chain_dim = jnp.exp(lppd_pointwise_chain_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all 1,2,3,..,10, 50, 100, 500 and every other 500 until truncate_samples\n",
    "sample_steps = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "sample_steps += [50, 100]\n",
    "sample_steps += list(range(500, truncate_samples, 500))\n",
    "sample_steps += [truncate_samples]\n",
    "sample_steps = np.unique(sample_steps)\n",
    "sample_steps = sample_steps[sample_steps <= truncate_samples]\n",
    "sample_steps = sample_steps.tolist()\n",
    "# calculate the rmse for each combination of samples and chains\n",
    "lppd_over_samples_and_chains = []\n",
    "for n_samples in sample_steps:\n",
    "    for i, n_chains in enumerate(good_chains):\n",
    "        inner = jnp.log(\n",
    "            jnp.mean(\n",
    "                ppd_pointwise_chain_dim[:, :n_samples, ...][good_chains[: i + 1], ...],\n",
    "                axis=[0, 1],\n",
    "            )\n",
    "        )\n",
    "        inner = inner[jnp.isfinite(inner)]\n",
    "        lppd_over_samples_and_chains.append(\n",
    "            jnp.nanmean(inner)\n",
    "        )\n",
    "# visualize the rmse over samples and chains using a heatmap\n",
    "lppd_over_samples_and_chains = np.array(lppd_over_samples_and_chains).reshape(\n",
    "    len(sample_steps), len(good_chains)\n",
    ")\n",
    "lppd_over_samples_and_chains = lppd_over_samples_and_chains[::-1, :]\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    lppd_over_samples_and_chains,\n",
    "    xticklabels=[c for c in range(1, len(good_chains)+1)],\n",
    "    yticklabels=sample_steps[::-1],\n",
    "    cmap='YlGn',\n",
    ")\n",
    "# find the indices of the minimum value in the heatmap\n",
    "min_idx = np.unravel_index(\n",
    "    np.nanargmax(lppd_over_samples_and_chains),\n",
    "    lppd_over_samples_and_chains.shape,\n",
    ")\n",
    "print(min_idx)\n",
    "# annotate with a red cross\n",
    "plt.scatter(min_idx[1]+0.5, min_idx[0]+0.5, marker='x', color='white')\n",
    "plt.xlabel('Number of Chains')\n",
    "plt.ylabel('Number of Samples (Non-Linear!)')\n",
    "plt.title(\n",
    "    (\n",
    "        'LPPD over Samples and'\n",
    "        ' Chains (Higher is better)'\n",
    "    )\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lppd_over_samples_and_chains_df = pd.DataFrame(\n",
    "    lppd_over_samples_and_chains,\n",
    "    index=sample_steps[::-1],\n",
    "    columns=good_chains,\n",
    ")\n",
    "lppd_over_samples_and_chains_df.to_csv(\n",
    "    f'../paper_bde/practical_sbi/chains_samples_grid/{DATASET}_lppd_over_samples_and_chains.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration assessment\n",
    "\n",
    "Again, save the data for better plotting in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in [0.25, 0.5, 0.75, 0.9, 0.98]:\n",
    "    hpdi_preds = hpdi(preds[good_chains_pred_indices], q)\n",
    "    acc_hpdi = jnp.mean(\n",
    "        (hpdi_preds[0, :] <= Y_val.squeeze())\n",
    "        & (hpdi_preds[1, :] >= Y_val.squeeze())\n",
    "    )\n",
    "    print(f'Accuracy of {int(q*100)}% HPDI: {acc_hpdi:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "colors = sns.color_palette('YlGn', 5)\n",
    "quantiles =  np.linspace(0.1, 0.9, 9)\n",
    "quantiles = np.concatenate([np.array([0.01, 0.25, 0.05]), quantiles, np.array([0.95, 0.99])])\n",
    "coverage_df = pd.DataFrame()\n",
    "# reverse the colors\n",
    "colors = colors[::-1]\n",
    "for trunc in [1, 10, 100, 1000, config[\"n_samples\"]]:\n",
    "    nominal_coverage_vals = []\n",
    "    for q in quantiles:\n",
    "        hpdi_preds = hpdi(preds_chain_dim[good_chains, :, :][:, :trunc, :].reshape(-1, preds.shape[1]), q)\n",
    "        nominal_coverage_vals.append(jnp.mean(\n",
    "            (hpdi_preds[0, :] <= Y_val.squeeze())\n",
    "            & (hpdi_preds[1, :] >= Y_val.squeeze())\n",
    "        ))\n",
    "    coverage_df_temp = pd.DataFrame({\n",
    "        \"quantiles\": quantiles,\n",
    "        \"empirical_coverage\": nominal_coverage_vals,\n",
    "        \"truncation\": trunc,\n",
    "        \"type\": \"samples\"\n",
    "    })\n",
    "    coverage_df = pd.concat([coverage_df, coverage_df_temp])\n",
    "    plt.plot(\n",
    "        quantiles, \n",
    "        nominal_coverage_vals, \n",
    "        label=f'{trunc} samples',\n",
    "        color=colors.pop(),\n",
    "        marker='o',\n",
    "        markerfacecolor='black',\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel('Nominal Coverage Level')\n",
    "plt.ylabel('Observed Coverage Level')\n",
    "plt.title('Nominal Coverage of PP Credibility Intervals')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.fill_between([0, 1], [0, 0], [0, 1], color='red', alpha=0.2)\n",
    "plt.annotate(\n",
    "    'Overconfidence',\n",
    "    xy=(0.75, 0.25),\n",
    "    xytext=(0.75, 0.25),\n",
    "    ha='center',\n",
    "    va = 'center',\n",
    "    color='red',\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "colors = sns.color_palette('YlGn', 5)\n",
    "quantiles =  np.array([0.01, 0.025, 0.05, 0.1])\n",
    "# reverse the colors\n",
    "colors = colors[::-1]\n",
    "for trunc in [1, 10, 100, 1000, config[\"n_samples\"]]: \n",
    "    nominal_coverage_vals = []\n",
    "    for q in quantiles:\n",
    "        hpdi_preds = hpdi(preds_chain_dim[good_chains, :, :][:, :trunc, :].reshape(-1, preds.shape[1]), q)\n",
    "        nominal_coverage_vals.append(jnp.mean(\n",
    "            (hpdi_preds[0, :] <= Y_val.squeeze())\n",
    "            & (hpdi_preds[1, :] >= Y_val.squeeze())\n",
    "        ))\n",
    "    plt.plot(\n",
    "        quantiles, \n",
    "        nominal_coverage_vals, \n",
    "        label=f'{trunc} samples',\n",
    "        color=colors.pop(),\n",
    "        marker='o',\n",
    "        markerfacecolor='black',\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel('Nominal Coverage Level')\n",
    "plt.ylabel('Observed Coverage Level')\n",
    "plt.title('Nominal Coverage of PP Credibility Intervals')\n",
    "plt.ylim([0, 0.11])\n",
    "plt.xlim([0, 0.11])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.plot([0, 0.11], [0, 0.11], color='black', linestyle='--')\n",
    "plt.fill_between([0, 0.11], [0, 0], [0, 0.11], color='red', alpha=0.2)\n",
    "plt.xticks(quantiles)\n",
    "plt.annotate(\n",
    "    'Overconfidence',\n",
    "    xy=(0.08, 0.03),\n",
    "    xytext=(0.08, 0.03),\n",
    "    ha='center',\n",
    "    va = 'center',\n",
    "    color='red',\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "colors = sns.color_palette('YlGn', 5)\n",
    "quantiles =  np.linspace(0.1, 0.9, 9)\n",
    "quantiles = np.concatenate([np.array([0.01, 0.025, 0.05]), quantiles, np.array([0.95, 0.99])])\n",
    "colors = colors[::-1]\n",
    "trunc = 100\n",
    "for chains in [1, 2, 4, 8, 10]: \n",
    "    nominal_coverage_vals = []\n",
    "    for q in quantiles:\n",
    "        hpdi_preds = hpdi(preds_chain_dim[good_chains, ...][:chains, :, :][:, :trunc, :].reshape(-1, preds.shape[1]), q)\n",
    "        nominal_coverage_vals.append(jnp.mean(\n",
    "            (hpdi_preds[0, :] <= Y_val.squeeze())\n",
    "            & (hpdi_preds[1, :] >= Y_val.squeeze())\n",
    "        ))\n",
    "    coverage_df_temp = pd.DataFrame({\n",
    "        \"quantiles\": quantiles,\n",
    "        \"empirical_coverage\": nominal_coverage_vals,\n",
    "        \"truncation\": chains,\n",
    "        \"type\": f\"chains_{trunc}samples\"\n",
    "    })\n",
    "    coverage_df = pd.concat([coverage_df, coverage_df_temp])\n",
    "    plt.plot(\n",
    "        quantiles, \n",
    "        nominal_coverage_vals, \n",
    "        label=f'{chains} Chains',\n",
    "        color=colors.pop(),\n",
    "        marker='o',\n",
    "        markerfacecolor='black',\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel('Nominal Coverage Level')\n",
    "plt.ylabel('Observed Coverage Level')\n",
    "plt.title(f'Nominal Coverage of PP Credibility Intervals ({trunc} Samples)')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.fill_between([0, 1], [0, 0], [0, 1], color='red', alpha=0.2)\n",
    "plt.annotate(\n",
    "    'Overconfidence',\n",
    "    xy=(0.75, 0.25),\n",
    "    xytext=(0.75, 0.25),\n",
    "    ha='center',\n",
    "    va = 'center',\n",
    "    color='red',\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df.to_csv(\n",
    "    f'../paper_bde/practical_sbi/calibration/{DATASET}_calibration.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_posterior_samples = {\n",
    "    k: v[good_chains, :truncate_samples, ...] for k, v in posterior_samples.items()\n",
    "}\n",
    "interchain_means_normal = pp_interchain_means(\n",
    "    trunc_posterior_samples, model, X_val\n",
    ")\n",
    "fig, ax = visualize_pp_chain_means(interchain_means_normal, 100, show=False)\n",
    "ax.set_xticklabels([str(i) for i in good_chains])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_chains_posterior_samples = {\n",
    "    k: v[good_chains, ...] for k, v in posterior_samples.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {}\n",
    "for param in good_chains_posterior_samples.keys():\n",
    "    all_params[param] = calculate_diagnostics(\n",
    "        good_chains_posterior_samples,\n",
    "        param,\n",
    "        truncate_samples,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_conv_diag_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ESS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list = list(all_params.keys())\n",
    "weight_parameters = [p for p in parameter_list if 'W' in p]\n",
    "bias_parameters = [p for p in parameter_list if 'b' in p]\n",
    "\n",
    "# calculate the average ESS for parameter layer\n",
    "ess_per_layer = {}\n",
    "bias_ess_per_layer = {}\n",
    "sd_ess_per_layer = {}\n",
    "sd_bias_ess_per_layer = {}\n",
    "for layer in range(1, len(weight_parameters)+1):\n",
    "    ess_per_layer[layer] = float(np.mean(all_params[f\"W{layer}\"][1]))\n",
    "    bias_ess_per_layer[layer] = float(np.mean(all_params[f\"b{layer}\"][1]))\n",
    "    sd_ess_per_layer[layer] = float(np.std(all_params[f\"W{layer}\"][1]))\n",
    "    sd_bias_ess_per_layer[layer] = float(np.std(all_params[f\"b{layer}\"][1]))\n",
    "    parameter_conv_diag_df = pd.concat(\n",
    "        [\n",
    "            parameter_conv_diag_df, \n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"layer\": [layer],\n",
    "                    \"parameter\": [\"weight\"],\n",
    "                    \"metric\": [\"ess\"],\n",
    "                    \"mean\": [ess_per_layer[layer]],\n",
    "                    \"sd\": [sd_ess_per_layer[layer]],\n",
    "                }\n",
    "            ),\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"layer\": [layer],\n",
    "                    \"parameter\": [\"bias\"],\n",
    "                    \"metric\": [\"ess\"],\n",
    "                    \"mean\": [bias_ess_per_layer[layer]],\n",
    "                    \"sd\": [sd_bias_ess_per_layer[layer]],\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(ess_per_layer.keys()),\n",
    "    y=list(ess_per_layer.values()),\n",
    "    ax=ax,\n",
    "    label='Weight',\n",
    "    color='blue',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(bias_ess_per_layer.keys()),\n",
    "    y=list(bias_ess_per_layer.values()),\n",
    "    ax=ax,\n",
    "    label='Bias',\n",
    "    color='orange',\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(ess_per_layer.keys()),\n",
    "    list(ess_per_layer.values()),\n",
    "    yerr=list(sd_ess_per_layer.values()),\n",
    "    color='blue',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(bias_ess_per_layer.keys()),\n",
    "    list(bias_ess_per_layer.values()),\n",
    "    yerr=list(sd_bias_ess_per_layer.values()),\n",
    "    color='orange',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Parameter Layer')\n",
    "ax.set_ylabel('Average ESS')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat R$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params_rhat_stats = {}\n",
    "for param in all_params.keys():\n",
    "    paramdict = all_params[param][0]\n",
    "    for key in paramdict.keys():\n",
    "        all_params_rhat_stats[key] = np.concatenate(\n",
    "            [all_params_rhat_stats[key] , paramdict[key].reshape(-1)]\n",
    "        ) if key in all_params_rhat_stats.keys() else paramdict[key].reshape(-1)\n",
    "\n",
    "all_rhats = []\n",
    "for param in all_params.keys():\n",
    "    all_rhats.append(all_params[param][0]['rhat'].flatten())\n",
    "all_rhats = np.concatenate(all_rhats).flatten()\n",
    "# save to csv\n",
    "pd.DataFrame(all_rhats).to_csv(\n",
    "    f'../paper_bde/practical_sbi/convergence/{DATASET}_{parameter_conv_diag_df[\"layer\"].max()}layers_rhat_params.csv'\n",
    ")\n",
    "visualize_rhat(all_params_rhat_stats)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerwise_params_rhat_stats_weights = {}\n",
    "layerwise_params_rhat_stats_biases = {}\n",
    "for layer in range(1, len(weight_parameters)+1):\n",
    "    layerwise_params_rhat_stats_weights[layer] = {}\n",
    "    layerwise_params_rhat_stats_biases[layer] = {}\n",
    "    for param in [p for p in list(all_params.keys()) if (str(layer) in p) and ('W' in p)]:\n",
    "        paramdict = all_params[param][0]\n",
    "        for key in paramdict.keys():\n",
    "            layerwise_params_rhat_stats_weights[layer][key] = np.concatenate(\n",
    "                [layerwise_params_rhat_stats_weights[layer][key] , paramdict[key].reshape(-1)]\n",
    "            ) if key in layerwise_params_rhat_stats_weights[layer].keys() else paramdict[key].reshape(-1)\n",
    "    print(f\"Layer {layer} - Weights\")\n",
    "    display(visualize_rhat(layerwise_params_rhat_stats_weights[layer]))\n",
    "    for param in [p for p in list(all_params.keys()) if (str(layer) in p) and ('b' in p)]:\n",
    "        paramdict = all_params[param][0]\n",
    "        for key in paramdict.keys():\n",
    "            layerwise_params_rhat_stats_biases[layer][key] = np.concatenate(\n",
    "                [layerwise_params_rhat_stats_biases[layer][key] , paramdict[key].reshape(-1)]\n",
    "            ) if key in layerwise_params_rhat_stats_biases[layer].keys() else paramdict[key].reshape(-1)\n",
    "    print(f\"Layer {layer} - Biases\")\n",
    "    display(visualize_rhat(layerwise_params_rhat_stats_biases[layer]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical R-hat and chainwise R-hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerwise_rhat_w = [\n",
    "    jnp.mean(layerwise_params_rhat_stats_weights[layer][\"rhat\"]).item() for layer in layerwise_params_rhat_stats_weights.keys()\n",
    "]\n",
    "layerwise_rhat_sd_w = [\n",
    "    jnp.std(layerwise_params_rhat_stats_weights[layer][\"rhat\"]).item() for layer in layerwise_params_rhat_stats_weights.keys()\n",
    "]\n",
    "layerwise_split_rhat_w = [\n",
    "    jnp.mean(layerwise_params_rhat_stats_weights[layer][\"split_chain_rhat\"]).item() for layer in layerwise_params_rhat_stats_weights.keys()\n",
    "]\n",
    "layerwise_split_rhat_sd_w = [\n",
    "    jnp.std(layerwise_params_rhat_stats_weights[layer][\"split_chain_rhat\"]).item() for layer in layerwise_params_rhat_stats_weights.keys()\n",
    "]\n",
    "layerwise_rhat_b = [\n",
    "    jnp.mean(layerwise_params_rhat_stats_biases[layer][\"rhat\"]).item() for layer in layerwise_params_rhat_stats_biases.keys()\n",
    "]\n",
    "layerwise_rhat_sd_b = [\n",
    "    jnp.std(layerwise_params_rhat_stats_biases[layer][\"rhat\"]).item() for layer in layerwise_params_rhat_stats_biases.keys()\n",
    "]\n",
    "layerwise_split_rhat_b = [\n",
    "    jnp.mean(layerwise_params_rhat_stats_biases[layer][\"split_chain_rhat\"]).item() for layer in layerwise_params_rhat_stats_biases.keys()\n",
    "]\n",
    "layerwise_split_rhat_sd_b = [\n",
    "    jnp.std(layerwise_params_rhat_stats_biases[layer][\"split_chain_rhat\"]).item() for layer in layerwise_params_rhat_stats_biases.keys()\n",
    "]\n",
    "for layer in layerwise_params_rhat_stats_weights.keys():\n",
    "    parameter_conv_diag_df = pd.concat(\n",
    "        [\n",
    "            parameter_conv_diag_df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"layer\": [layer],\n",
    "                    \"parameter\": [\"weight\"],\n",
    "                    \"metric\": [\"rhat\"],\n",
    "                    \"mean\": [layerwise_rhat_w[layer-1]],\n",
    "                    \"sd\": [layerwise_rhat_sd_w[layer-1]],\n",
    "                }\n",
    "            ),\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"layer\": [layer],\n",
    "                    \"parameter\": [\"bias\"],\n",
    "                    \"metric\": [\"rhat\"],\n",
    "                    \"mean\": [layerwise_rhat_b[layer-1]],\n",
    "                    \"sd\": [layerwise_rhat_sd_b[layer-1]],\n",
    "                }\n",
    "            ),\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"layer\": [layer],\n",
    "                    \"parameter\": [\"weight\"],\n",
    "                    \"metric\": [\"split_chain_rhat\"],\n",
    "                    \"mean\": [layerwise_split_rhat_w[layer-1]],\n",
    "                    \"sd\": [layerwise_split_rhat_sd_w[layer-1]],\n",
    "                }\n",
    "            ),\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"layer\": [layer],\n",
    "                    \"parameter\": [\"bias\"],\n",
    "                    \"metric\": [\"split_chain_rhat\"],\n",
    "                    \"mean\": [layerwise_split_rhat_b[layer-1]],\n",
    "                    \"sd\": [layerwise_split_rhat_sd_b[layer-1]],\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(layerwise_params_rhat_stats_weights.keys()),\n",
    "    y=layerwise_rhat_w,\n",
    "    ax=ax,\n",
    "    label='$\\widehat{R}$ of Weights',\n",
    "    color='#06238f',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(layerwise_params_rhat_stats_biases.keys()),\n",
    "    y=layerwise_rhat_b,\n",
    "    ax=ax,\n",
    "    label='$\\widehat{R}$ of Biases',\n",
    "    color='#2e59f2',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(layerwise_params_rhat_stats_weights.keys()),\n",
    "    y=layerwise_split_rhat_w,\n",
    "    ax=ax,\n",
    "    label='Chainwise $\\widehat{R}$ of Weights',\n",
    "    color='#035c0c',\n",
    "    linestyle='--',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(layerwise_params_rhat_stats_biases.keys()),\n",
    "    y=layerwise_split_rhat_b,\n",
    "    ax=ax,\n",
    "    label='Chainwise $\\widehat{R}$ of Biases',\n",
    "    color='#26bf36',\n",
    "    linestyle='--',\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(layerwise_params_rhat_stats_weights.keys()),\n",
    "    layerwise_rhat_w,\n",
    "    yerr=layerwise_rhat_sd_w,\n",
    "    color='#06238f',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(layerwise_params_rhat_stats_biases.keys()),\n",
    "    layerwise_rhat_b,\n",
    "    yerr=layerwise_rhat_sd_b,\n",
    "    color='#2e59f2',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(layerwise_params_rhat_stats_weights.keys()),\n",
    "    layerwise_split_rhat_w,\n",
    "    yerr=layerwise_split_rhat_sd_w,\n",
    "    color='#035c0c',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(layerwise_params_rhat_stats_biases.keys()),\n",
    "    layerwise_split_rhat_b,\n",
    "    yerr=layerwise_split_rhat_sd_b,\n",
    "    color='#26bf36',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=1)\n",
    "ax.set_xlabel('Hidden Layer')\n",
    "ax.set_ylabel('')\n",
    "plt.close(fig)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the decomposition of the classic Rhat across layers (parameters) for both weights and biases (only use good chains)\n",
    "between_chain_var_w = {}\n",
    "within_chain_var_w = {}\n",
    "between_chain_var_b = {}\n",
    "within_chain_var_b = {}\n",
    "\n",
    "for param in good_chains_posterior_samples.keys():\n",
    "    if 'W' in param:\n",
    "        between_chain_var_w[param] = good_chains_posterior_samples[param].mean(axis = 1).std(axis = 0)**2\n",
    "        within_chain_var_w[param] = (good_chains_posterior_samples[param].std(axis = 1)**2).mean(axis = 0)\n",
    "    elif 'b' in param:\n",
    "        between_chain_var_b[param] = good_chains_posterior_samples[param].mean(axis = 1).std(axis = 0)\n",
    "        within_chain_var_b[param] = good_chains_posterior_samples[param].std(axis = 1).mean(axis = 0)\n",
    "between_chain_var_w_mean = {}\n",
    "between_chain_var_w_std = {}\n",
    "within_chain_var_w_mean = {}\n",
    "within_chain_var_w_std = {}\n",
    "between_chain_var_b_mean = {}\n",
    "between_chain_var_b_std = {}\n",
    "within_chain_var_b_mean = {}\n",
    "within_chain_var_b_std = {}\n",
    "for layer in range(1, len(weight_parameters)+1):\n",
    "    between_chain_var_w_mean[layer] = np.mean(between_chain_var_w[f\"W{layer}\"]).item()\n",
    "    between_chain_var_w_std[layer] = np.std(between_chain_var_w[f\"W{layer}\"]).item()\n",
    "    within_chain_var_w_mean[layer] = np.mean(within_chain_var_w[f\"W{layer}\"]).item()\n",
    "    within_chain_var_w_std[layer] = np.std(within_chain_var_w[f\"W{layer}\"]).item()\n",
    "    between_chain_var_b_mean[layer] = np.mean(between_chain_var_b[f\"b{layer}\"]).item()\n",
    "    between_chain_var_b_std[layer] = np.std(between_chain_var_b[f\"b{layer}\"]).item()\n",
    "    within_chain_var_b_mean[layer] = np.mean(within_chain_var_b[f\"b{layer}\"]).item()\n",
    "    within_chain_var_b_std[layer] = np.std(within_chain_var_b[f\"b{layer}\"]).item()\n",
    "    parameter_conv_diag_df = pd.concat([\n",
    "        parameter_conv_diag_df,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"layer\": [layer],\n",
    "                \"parameter\": [\"weight\"],\n",
    "                \"metric\": [\"between_chain_var\"],\n",
    "                \"mean\": [between_chain_var_w_mean[layer]],\n",
    "                \"sd\": [between_chain_var_w_std[layer]],\n",
    "            }\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"layer\": [layer],\n",
    "                \"parameter\": [\"bias\"],\n",
    "                \"metric\": [\"between_chain_var\"],\n",
    "                \"mean\": [between_chain_var_b_mean[layer]],\n",
    "                \"sd\": [between_chain_var_b_std[layer]],\n",
    "            }\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"layer\": [layer],\n",
    "                \"parameter\": [\"weight\"],\n",
    "                \"metric\": [\"within_chain_var\"],\n",
    "                \"mean\": [within_chain_var_w_mean[layer]],\n",
    "                \"sd\": [within_chain_var_w_std[layer]],\n",
    "            }\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"layer\": [layer],\n",
    "                \"parameter\": [\"bias\"],\n",
    "                \"metric\": [\"within_chain_var\"],\n",
    "                \"mean\": [within_chain_var_b_mean[layer]],\n",
    "                \"sd\": [within_chain_var_b_std[layer]],\n",
    "            }\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(between_chain_var_w_mean.keys()),\n",
    "    y=list(between_chain_var_w_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Weight',\n",
    "    color='#06238f',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(between_chain_var_b_mean.keys()),\n",
    "    y=list(between_chain_var_b_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Bias',\n",
    "    color='#2e59f2',\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(between_chain_var_w_mean.keys()),\n",
    "    list(between_chain_var_w_mean.values()),\n",
    "    yerr=list(between_chain_var_w_std.values()),\n",
    "    color='#06238f',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(between_chain_var_b_mean.keys()),\n",
    "    list(between_chain_var_b_mean.values()),\n",
    "    yerr=list(between_chain_var_b_std.values()),\n",
    "    color='#2e59f2',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Avg. Between Chain Variance')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.close(fig)\n",
    "display(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(within_chain_var_w_mean.keys()),\n",
    "    y=list(within_chain_var_w_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Weight',\n",
    "    color='#06238f',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(within_chain_var_b_mean.keys()),\n",
    "    y=list(within_chain_var_b_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Bias',\n",
    "    color='#2e59f2',\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(within_chain_var_w_mean.keys()),\n",
    "    list(within_chain_var_w_mean.values()),\n",
    "    yerr=list(within_chain_var_w_std.values()),\n",
    "    color='#06238f',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(within_chain_var_b_mean.keys()),\n",
    "    list(within_chain_var_b_mean.values()),\n",
    "    yerr=list(within_chain_var_b_std.values()),\n",
    "    color='#2e59f2',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Avg. Within Chain Variance')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.close(fig)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the absolut value of sampled weights and their standard deviation across layers (parameters) for both weights and biases (only use good chains)\n",
    "abs_weight_samples = {}\n",
    "abs_bias_samples = {}\n",
    "for param in good_chains_posterior_samples.keys():\n",
    "    if 'W' in param:\n",
    "        abs_weight_samples[param] = np.abs(good_chains_posterior_samples[param])\n",
    "    elif 'b' in param:\n",
    "        abs_bias_samples[param] = np.abs(good_chains_posterior_samples[param])\n",
    "abs_weight_samples_mean = {}\n",
    "abs_weight_samples_std = {}\n",
    "abs_bias_samples_mean = {}\n",
    "abs_bias_samples_std = {}\n",
    "for layer in range(1, len(weight_parameters)+1):\n",
    "    abs_weight_samples_mean[layer] = np.mean(abs_weight_samples[f\"W{layer}\"])\n",
    "    abs_weight_samples_std[layer] = np.std(abs_weight_samples[f\"W{layer}\"])\n",
    "    abs_bias_samples_mean[layer] = np.mean(abs_bias_samples[f\"b{layer}\"])\n",
    "    abs_bias_samples_std[layer] = np.std(abs_bias_samples[f\"b{layer}\"])\n",
    "    parameter_conv_diag_df = pd.concat([\n",
    "        parameter_conv_diag_df,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"layer\": [layer],\n",
    "                \"parameter\": [\"weight\"],\n",
    "                \"metric\": [\"abs_samples_mean\"],\n",
    "                \"mean\": [abs_weight_samples_mean[layer]],\n",
    "                \"sd\": [abs_weight_samples_std[layer]],\n",
    "            }\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"layer\": [layer],\n",
    "                \"parameter\": [\"bias\"],\n",
    "                \"metric\": [\"abs_samples_mean\"],\n",
    "                \"mean\": [abs_bias_samples_mean[layer]],\n",
    "                \"sd\": [abs_bias_samples_std[layer]],\n",
    "            }\n",
    "        ),\n",
    "    ])\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(abs_weight_samples_mean.keys()),\n",
    "    y=list(abs_weight_samples_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Weight',\n",
    "    color='blue',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(abs_bias_samples_mean.keys()),\n",
    "    y=list(abs_bias_samples_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Bias',\n",
    "    color='orange',\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(abs_weight_samples_mean.keys()),\n",
    "    list(abs_weight_samples_mean.values()),\n",
    "    yerr=list(abs_weight_samples_std.values()),\n",
    "    color='blue',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(abs_bias_samples_mean.keys()),\n",
    "    list(abs_bias_samples_mean.values()),\n",
    "    yerr=list(abs_bias_samples_std.values()),\n",
    "    color='orange',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Average Absolute Value of Posterior (Parameter) Samples')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always save data to file for better plotting in R and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_conv_diag_df[\"data\"] = DATASET\n",
    "parameter_conv_diag_df[\"n_layers\"] = parameter_conv_diag_df[\"layer\"].max()\n",
    "parameter_conv_diag_df.to_csv(\n",
    "    f'../paper_bde/practical_sbi/convergence/{DATASET}_{parameter_conv_diag_df[\"layer\"].max()}layers_parameter_conv.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_split_chain_rhat = split_chain_r_hat(\n",
    "    preds_chain_dim[good_chains, :truncate_samples, :],\n",
    "    n_splits=4,\n",
    ")\n",
    "fig_pp_rhat = visualize_pp_rhat(pp_split_chain_rhat)\n",
    "fig_pp_rhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all function space rhats as above\n",
    "pd.DataFrame(pp_split_chain_rhat[\"rhat\"].flatten()).to_csv(\n",
    "    f'../paper_bde/practical_sbi/convergence/{DATASET}_{parameter_conv_diag_df[\"layer\"].max()}layers_rhat_pp.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lppd_pointwise = model.get_lppd(X_val, Y_val, posterior_samples_raw, rolling=False)\n",
    "print(lppd_pointwise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lppd_pointwise_chain_dim = add_chain_dimension({'lppd': lppd_pointwise}, n_chains=exp_info[\"n_chains\"])['lppd']\n",
    "# only use the good chains\n",
    "lppd_pointwise_chain_dim = lppd_pointwise_chain_dim[good_chains, ...]\n",
    "ppd_pointwise_chain_dim = jnp.exp(lppd_pointwise_chain_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainwise_lpl = jnp.log(jnp.expand_dims(ppd_pointwise_chain_dim.mean(axis=2), axis=2))\n",
    "flat_lpl = chainwise_lpl[..., 0].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainwise_mean_lpl = chainwise_lpl[..., 0].mean(axis=1)\n",
    "chainwise_sd_lpl = chainwise_lpl[..., 0].std(axis=1)\n",
    "# visualize lineplot with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=good_chains,\n",
    "    y=chainwise_mean_lpl,\n",
    "    ax=ax,\n",
    "    color='blue',\n",
    ")\n",
    "ax.errorbar(\n",
    "    good_chains,\n",
    "    chainwise_mean_lpl,\n",
    "    yerr=chainwise_sd_lpl,\n",
    "    color='blue',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_xlabel('Chain')\n",
    "ax.set_xticks(good_chains)\n",
    "ax.set_ylabel('Chainwise LPL')\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize it as a colored traceplot\n",
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "chain_names = [f'chain_{i}' for i in range(len(good_chains))]\n",
    "colors = np.repeat(chain_names, truncate_samples, axis=0)\n",
    "function_space_df = pd.DataFrame({\n",
    "    \"sample\": np.tile(np.arange(truncate_samples), len(good_chains))+1,\n",
    "    \"base_val\": flat_lpl,\n",
    "    \"chain\": colors,\n",
    "    \"type\": \"lpl\",\n",
    "    \"rhat\": gelman_split_r_hat(chainwise_lpl, n_splits=2, rank_normalize=True).item(),\n",
    "})\n",
    "sns.lineplot(\n",
    "    x=np.arange(len(flat_lpl)),\n",
    "    y=jnp.exp(flat_lpl),\n",
    "    ax=ax,\n",
    "    hue = colors\n",
    ")\n",
    "# ylim 0 1\n",
    "# ax.set_ylim([0, 1])\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('LPL')\n",
    "ax.set_title('LPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelman_split_r_hat(chainwise_lpl, n_splits=2, rank_normalize=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in a rolling window fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the chains\n",
    "chainwise_rolling_lppd = []\n",
    "ppd_pointwise_chain_dim\n",
    "for chain in good_chains:\n",
    "    chain_cum_lppd = []\n",
    "    for samp in range(ppd_pointwise_chain_dim.shape[1]):\n",
    "        # only every 50th sample\n",
    "        if samp % 20 != 0:\n",
    "            continue\n",
    "        inner = jnp.log(\n",
    "            jnp.mean(\n",
    "                ppd_pointwise_chain_dim[chain, :samp+1, ...],\n",
    "                axis=0,\n",
    "            )\n",
    "        )\n",
    "        inner = inner[jnp.isfinite(inner)]\n",
    "        chain_cum_lppd.append(\n",
    "            jnp.mean(inner)\n",
    "        )\n",
    "    chainwise_rolling_lppd.append(\n",
    "        chain_cum_lppd\n",
    "    )\n",
    "chainwise_rolling_lppd = np.array(chainwise_rolling_lppd)\n",
    "chainwise_rolling_lppd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_rolling_lppd = flatten_chain_dimension({\"clppd\": chainwise_rolling_lppd})[\"clppd\"]\n",
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "chain_names = [f'chain_{i}' for i in range(len(good_chains))]\n",
    "colors = np.repeat(chain_names, chainwise_rolling_lppd.shape[1], axis=0)\n",
    "function_space_df = pd.concat([\n",
    "    function_space_df,\n",
    "    pd.DataFrame({\n",
    "        \"sample\": np.tile(np.arange(chainwise_rolling_lppd.shape[1]), len(good_chains))+1,\n",
    "        \"base_val\": flat_rolling_lppd,\n",
    "        \"chain\": colors,\n",
    "        \"type\": \"lppd\",\n",
    "        \"rhat\": np.nan,\n",
    "    })\n",
    "])\n",
    "sns.lineplot(\n",
    "    x=20*np.array(list(np.arange(1, chainwise_rolling_lppd.shape[1]+1)) * len(good_chains)),\n",
    "    y=flat_rolling_lppd,\n",
    "    ax=ax,\n",
    "    hue = colors\n",
    ")\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('LPPD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the rmse instead of the lppd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_rmse(preds, Y):\n",
    "    return jnp.sqrt(jnp.mean((preds.squeeze() - Y.squeeze())**2))\n",
    "\n",
    "mse_per_chain = jnp.apply_along_axis(pointwise_rmse, 2, preds_chain_dim, Y_val)\n",
    "mse_per_chain = mse_per_chain[good_chains, ...]\n",
    "mse_flat = mse_per_chain.reshape(-1)\n",
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "chain_names = [f'chain_{i}' for i in range(len(good_chains))]\n",
    "colors = np.repeat(chain_names, truncate_samples, axis=0)\n",
    "function_space_df = pd.concat([\n",
    "    function_space_df,\n",
    "    pd.DataFrame({\n",
    "        \"sample\": np.tile(np.arange(truncate_samples), len(good_chains))+1,\n",
    "        \"base_val\": mse_flat,\n",
    "        \"chain\": colors,\n",
    "        \"type\": \"rmse\",\n",
    "        \"rhat\": gelman_split_r_hat(jnp.expand_dims(mse_per_chain[good_chains, ...], 2), n_splits=2, rank_normalize=True).item(),\n",
    "    })\n",
    "])\n",
    "sns.lineplot(\n",
    "    x=np.arange(len(mse_flat)),\n",
    "    y=(mse_flat),\n",
    "    ax=ax,\n",
    "    hue = colors\n",
    ")\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('RMSE of each Posterior Sample induced Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelman_split_r_hat(jnp.expand_dims(mse_per_chain[good_chains, ...], 2), n_splits=2, rank_normalize=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_space_df.to_csv(\n",
    "    f'../paper_bde/practical_sbi/convergence/{DATASET}_{parameter_conv_diag_df[\"layer\"].max()}layers_function_space.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rmse for predictions with low and high and compare\n",
    "# to rmse with low rhat\n",
    "low_rhat = [i for i in range(preds_chain_dim.shape[2]) if pp_split_chain_rhat[\"rhat\"][i] < 1.1]\n",
    "print(f\"Number of predictions with low Rhat: {len(low_rhat)}\")\n",
    "rhat_low_rmse = np.sqrt(\n",
    "    mse(\n",
    "        preds_chain_dim[good_chains, :truncate_samples, :][:, :, \n",
    "            low_rhat\n",
    "        ],\n",
    "        Y_val[low_rhat, :],\n",
    "    )[0]\n",
    ")\n",
    "print(\"RMSE for predictions with low Rhat\")\n",
    "print(rhat_low_rmse)\n",
    "high_rhat = [i for i in range(preds_chain_dim.shape[2]) if pp_split_chain_rhat[\"rhat\"][i] >= 1.1]\n",
    "print(f\"Number of predictions with high Rhat: {len(high_rhat)}\")\n",
    "rhat_high_rmse = np.sqrt(\n",
    "    mse(\n",
    "        preds_chain_dim[good_chains, :truncate_samples, :][:, :, \n",
    "            high_rhat\n",
    "        ],\n",
    "        Y_val[high_rhat, :],\n",
    "    )[0]\n",
    ")\n",
    "print(\"RMSE for predictions with high Rhat\")\n",
    "print(rhat_high_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
