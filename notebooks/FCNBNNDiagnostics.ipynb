{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the posterior samples and their induced models from selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from IPython.display import display\n",
    "import jax.numpy as jnp\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import pandas as pd\n",
    "from numpyro.diagnostics import hpdi\n",
    "sys.path.append('../')\n",
    "from experiments.fcn_bnns.utils.analysis_utils import *\n",
    "from module_sandbox.utils import (  # noqa: E402\n",
    "    mse,\n",
    ")\n",
    "from module_sandbox.visualization.posterior_predictive import (  # noqa: E402\n",
    "    pp_interchain_means,\n",
    "    visualize_pp_chain_means,\n",
    ")\n",
    "from module_sandbox.utils import flatten_chain_dimension\n",
    "from experiments.fcn_bnns.utils.ui_utils import (  # noqa: E402\n",
    "    calculate_diagnostics,\n",
    "    plot_sample_paths,\n",
    "    visualize_ess,\n",
    "    visualize_pp_rhat,\n",
    "    visualize_rhat,\n",
    ")\n",
    "from module_sandbox.diagnostics.gelman import split_chain_r_hat, gelman_split_r_hat  # noqa: E402\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First main Experiments: `2023-12-22-16-22-58`\n",
    "\n",
    "- **Airfoil**: `exp54|airfoil.data|tanh|32-32-32|12|8000|False|NUTS_large|>>replication<<|1|Normal` Alternative: 16-16\n",
    "- **Concrete**: `exp112|concrete.data|tanh|16-16|12|8000|False|NUTS_large|>>replication<<|1|Normal`\n",
    "- **Energy**: `exp172|energy.data|tanh|16-16|12|8000|False|NUTS_large|>>replication<<|1|Normal` Alternative: 32-32-32\n",
    "- **Yacht**: `exp229|yacht.data|tanh|16-16|12|8000|False|NUTS_large|>>replication<<|1|Normal`\n",
    "\n",
    "Bikesharing: `2024-01-08-15-48-28`\n",
    "\n",
    "Use for Calibration as the validation set is large enough to get a good estimate also at small quantiles.\n",
    "`'bikesharing.data|tanh|16-16|12|4000|False|NUTS_large|{replication}|1|Normal'`\n",
    "\n",
    "Wide Experiments: \n",
    "\n",
    "- `2024-01-10-08-55-13` with `f'airfoil.data|tanh|10-10-10-10-10|8|800|False|NUTS_large|{replication}|0.701067|Laplace'`\n",
    "- `2024-01-16-13-57-52` with `f'airfoil.data|tanh|8-8-8-8-8-8|12|8000|False|NUTS_large|{replication}|1|Normal'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datet = \"2023-12-22-16-22-58\"\n",
    "# datet = \"2024-01-10-08-55-13\"\n",
    "# datet = \"2024-01-08-15-48-28\"\n",
    "# datet = \"2024-01-16-13-57-52\"\n",
    "CONFIG_PATH = f'../results/fcn_bnns/{datet}/config.yaml'\n",
    "DATA_PATH = '../data'\n",
    "replication = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names = get_exp_names(path=f\"../results/fcn_bnns/{datet}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f'airfoil.data|tanh|32-32-32|12|8000|False|NUTS_large|{replication}|1|Normal'\n",
    "exp_name = [ename for ename in exp_names if exp_name in ename][0]\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_info = extract_exp_info(exp_name)\n",
    "config = load_config(CONFIG_PATH)\n",
    "n_chains = int(exp_info['n_chains'])\n",
    "n_samples = int(exp_info['n_samples'])\n",
    "X_train, Y_train = load_data(exp_info, splittype='train', data_path=DATA_PATH)\n",
    "X_val, Y_val = load_data(exp_info, splittype='val', data_path=DATA_PATH)\n",
    "val_threshold = min(1000, X_val.shape[0])\n",
    "X_val = X_val[:val_threshold, :]\n",
    "Y_val = Y_val[:val_threshold, :]\n",
    "linear_regr, rf_regr = fit_baselines(X_train, Y_train)\n",
    "mse_linear, mse_rf = evaluate_baselines(X_val, Y_val, linear_regr, rf_regr)\n",
    "res_dict = {}\n",
    "res_dict['rmse_linear'] = np.sqrt(mse_linear)\n",
    "res_dict['rmse_rf'] = np.sqrt(mse_rf)\n",
    "posterior_samples, posterior_samples_raw = load_samples(exp_name, f'../results/fcn_bnns/{datet}')\n",
    "model = load_model(exp_name, f'../results/fcn_bnns/{datet}')\n",
    "preds_chain_dim, preds = get_posterior_predictive(\n",
    "    model, posterior_samples_raw, X_val, exp_info['n_chains']\n",
    ")\n",
    "rmse_per_chain = {}\n",
    "for i in range(preds_chain_dim.shape[0]):\n",
    "    rmse_per_chain[f'chain_{i}'] = np.sqrt(mse(preds_chain_dim[i], Y_val)[0])\n",
    "rmse_table = pd.DataFrame(rmse_per_chain, index=['RMSE']).T\n",
    "bad_chains = rmse_table[rmse_table['RMSE'] > np.sqrt(mse_linear)].index\n",
    "bad_chains = bad_chains.str.split('_').str[1].astype(int).values\n",
    "bad_chains = bad_chains.tolist()\n",
    "good_chains = [i for i in range(n_chains) if i not in bad_chains]\n",
    "if len(good_chains) > 0:\n",
    "    good_chains_pred_indices = np.concatenate(\n",
    "        [np.arange(n_samples) + (n_samples * i) for i in good_chains]\n",
    "    )\n",
    "    good_chains_pred_indices_100 = np.concatenate(\n",
    "        [np.arange(100) + (n_samples * i) for i in good_chains]\n",
    "    )\n",
    "res_dict['n_bad_chains'] = len(bad_chains)\n",
    "res_dict['n_good_chains'] = len(good_chains)\n",
    "if len(good_chains) == 0:\n",
    "    res_dict['rmse_good_chains'] = np.nan\n",
    "    res_dict['rmse_good_chains_100'] = np.nan\n",
    "    res_dict['acc_90hpdi'] = np.nan\n",
    "    res_dict['acc_90hpdi_100'] = np.nan\n",
    "else:\n",
    "    # RMSE\n",
    "    res_dict['rmse_good_chains'] = np.sqrt(\n",
    "        mse(preds[good_chains_pred_indices, :], Y_val)[0]\n",
    "    )\n",
    "    res_dict['rmse_good_chains_100'] = np.sqrt(\n",
    "        mse(preds[good_chains_pred_indices_100, :], Y_val)[0]\n",
    "    )\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_samples = config[\"n_samples\"]\n",
    "# truncate_samples = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_per_chain = {}\n",
    "for i in range(preds_chain_dim.shape[0]):\n",
    "    rmse_per_chain[f'chain_{i}'] = np.sqrt(mse(preds_chain_dim[i], Y_val)[0])\n",
    "rmse_table = pd.DataFrame(rmse_per_chain, index=['RMSE']).T\n",
    "bad_chains = rmse_table[rmse_table['RMSE'] > res_dict[\"rmse_linear\"]].index\n",
    "bad_chains = bad_chains.str.split('_').str[1].astype(int).values\n",
    "bad_chains = bad_chains.tolist()\n",
    "good_chains = [i for i in range(n_chains) if i not in bad_chains]\n",
    "n_samples = config['n_samples']\n",
    "good_chains_pred_indices = np.concatenate(\n",
    "    [np.arange(truncate_samples) + (n_samples * i) for i in good_chains]\n",
    ")\n",
    "rmse_table = rmse_table.sort_values(by='RMSE', ascending=True)\n",
    "# all entries of the df that are > np.sqrt(mse_linear_model) should get a red\n",
    "# background\n",
    "def color_cells(x):\n",
    "    \"\"\"Color the cells of the table.\"\"\"\n",
    "    return 'background-color: red' if x > res_dict[\"rmse_linear\"] else ''\n",
    "rmse_table = rmse_table.style.map(color_cells)\n",
    "rmse_table = rmse_table.format('{:.3f}')\n",
    "rmse_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all 1,2,3,..,10, 50, 100, 500 and every other 500 until truncate_samples\n",
    "sample_steps = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "sample_steps += [50, 100]\n",
    "sample_steps += list(range(500, truncate_samples, 500))\n",
    "sample_steps += [truncate_samples]\n",
    "sample_steps = np.unique(sample_steps)\n",
    "sample_steps = sample_steps[sample_steps <= truncate_samples]\n",
    "sample_steps = sample_steps.tolist()\n",
    "# calculate the rmse for each combination of samples and chains\n",
    "rmse_over_samples_and_chains = []\n",
    "for n_samples in sample_steps:\n",
    "    for i, n_chains in enumerate(good_chains):\n",
    "        rmse_over_samples_and_chains.append(\n",
    "            np.sqrt(\n",
    "                mse(\n",
    "                    preds_chain_dim[good_chains[: i + 1], :n_samples, ...].reshape(\n",
    "                        -1, *preds_chain_dim.shape[2:]\n",
    "                    ),\n",
    "                    Y_val,\n",
    "                )[0]\n",
    "            )\n",
    "        )\n",
    "# visualize the rmse over samples and chains using a heatmap\n",
    "rmse_over_samples_and_chains = np.array(rmse_over_samples_and_chains).reshape(\n",
    "    len(sample_steps), len(good_chains)\n",
    ")\n",
    "rmse_over_samples_and_chains = rmse_over_samples_and_chains[::-1, :]\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    rmse_over_samples_and_chains,\n",
    "    xticklabels=[c for c in range(1, len(good_chains)+1)],\n",
    "    yticklabels=sample_steps[::-1],\n",
    "    cmap='YlGn_r',\n",
    ")\n",
    "# find the indices of the minimum value in the heatmap\n",
    "min_idx = np.unravel_index(\n",
    "    np.nanargmin(rmse_over_samples_and_chains),\n",
    "    rmse_over_samples_and_chains.shape,\n",
    ")\n",
    "print(min_idx)\n",
    "# annotate with a red cross\n",
    "plt.scatter(min_idx[1]+0.5, min_idx[0]+0.5, marker='x', color='white')\n",
    "plt.xlabel('Number of Chains')\n",
    "plt.ylabel('Number of Samples (Non-Linear!)')\n",
    "plt.title(\n",
    "    (\n",
    "        'RMSE over Samples and'\n",
    "        ' Chains (Lower is better)'\n",
    "    )\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lppd_pointwise = model.get_lppd(X_val, Y_val, posterior_samples_raw, rolling=False)\n",
    "lppd_pointwise_chain_dim = add_chain_dimension({'lppd': lppd_pointwise}, n_chains=exp_info[\"n_chains\"])['lppd']\n",
    "ppd_pointwise_chain_dim = jnp.exp(lppd_pointwise_chain_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_pointwise_chain_dim.mean(axis=[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhat_chain_filtering(ppd_samples: jnp.array, threshold: float = 1.1) -> tuple[list, list, float]:\n",
    "    \"\"\"\n",
    "    Filter chains based on the split rhat of the pointwise likelihood.\n",
    "\n",
    "    Args:\n",
    "        ppd_samples (jnp.array): Evaluations of the pointwise posterior predictive on a testset. \n",
    "        Dimension: (n_chains, n_samples, n_test_samples).\n",
    "        threshold (float, optional): Threshold for the split rhat. Defaults to 1.1.\n",
    "\n",
    "    Returns:\n",
    "        tuple(list, list, float): Good chains, bad chains, rhat.\n",
    "    \"\"\"\n",
    "    pdd_over_chains = jnp.log(ppd_samples.mean(axis=2))\n",
    "    good_chains = [i for i in range(pdd_over_chains.shape[0])]\n",
    "    bad_chains = []\n",
    "    # constant chains are directly assigned to bad chains\n",
    "    constant_chains = []\n",
    "    for i in range(pdd_over_chains.shape[0]):\n",
    "        if jnp.all(pdd_over_chains[i, :] == pdd_over_chains[i, 0]):\n",
    "            constant_chains.append(i)\n",
    "    if len(constant_chains) > 0:\n",
    "        print(f'Constant chains: {constant_chains}')\n",
    "        bad_chains += constant_chains\n",
    "        good_chains = [i for i in good_chains if i not in bad_chains]\n",
    "\n",
    "    rhat = threshold + 1\n",
    "    while rhat > threshold and len(good_chains) >= 2:\n",
    "        drop_candidates = {}\n",
    "        for drop_candidate in good_chains:\n",
    "            chains_without_drop_candidate = [\n",
    "                i for i in good_chains if i != drop_candidate\n",
    "            ]\n",
    "            drop_candidates[drop_candidate] = gelman_split_r_hat(\n",
    "                pdd_over_chains[chains_without_drop_candidate, :],\n",
    "                n_splits=2,\n",
    "            ).item()\n",
    "        # drop the chain with the best rhat\n",
    "        drop_chain = min(drop_candidates, key=drop_candidates.get)\n",
    "        bad_chains.append(drop_chain)\n",
    "        good_chains = [i for i in good_chains if i != drop_chain]\n",
    "        rhat = drop_candidates[drop_chain]\n",
    "    \n",
    "    if rhat > threshold:\n",
    "        good_chains = []\n",
    "        bad_chains = [i for i in range(pdd_over_chains.shape[0])]\n",
    "        print('All chains had to be dropped!')\n",
    "    return good_chains, bad_chains, rhat\n",
    "            \n",
    "good_chains, bad_chains, rhat_lppd_good_chains = rhat_chain_filtering(ppd_pointwise_chain_dim, 1.2)           \n",
    "print(f'Good chains: {good_chains}')\n",
    "print(f'Bad chains: {bad_chains}')\n",
    "print(f'rhat: {rhat_lppd_good_chains:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count infs\n",
    "(ppd_pointwise_chain_dim==0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all 1,2,3,..,10, 50, 100, 500 and every other 500 until truncate_samples\n",
    "sample_steps = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "sample_steps += [50, 100]\n",
    "sample_steps += list(range(500, truncate_samples, 500))\n",
    "sample_steps += [truncate_samples]\n",
    "sample_steps = np.unique(sample_steps)\n",
    "sample_steps = sample_steps[sample_steps <= truncate_samples]\n",
    "sample_steps = sample_steps.tolist()\n",
    "# calculate the rmse for each combination of samples and chains\n",
    "lppd_over_samples_and_chains = []\n",
    "for n_samples in sample_steps:\n",
    "    for i, n_chains in enumerate(good_chains):\n",
    "        lppd_over_samples_and_chains.append(\n",
    "            jnp.log(\n",
    "                jnp.mean(\n",
    "                    ppd_pointwise_chain_dim[:, :n_samples, ...][good_chains[: i + 1], ...],\n",
    "                    axis=[0, 1],\n",
    "                )\n",
    "            ).sum()\n",
    "        )\n",
    "# visualize the rmse over samples and chains using a heatmap\n",
    "lppd_over_samples_and_chains = np.array(lppd_over_samples_and_chains).reshape(\n",
    "    len(sample_steps), len(good_chains)\n",
    ")\n",
    "lppd_over_samples_and_chains = lppd_over_samples_and_chains[::-1, :]\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "\n",
    "sns.heatmap(\n",
    "    lppd_over_samples_and_chains,\n",
    "    xticklabels=[c for c in range(1, len(good_chains)+1)],\n",
    "    yticklabels=sample_steps[::-1],\n",
    "    cmap='YlGn',\n",
    "    norm=LogNorm()\n",
    ")\n",
    "# find the indices of the minimum value in the heatmap\n",
    "min_idx = np.unravel_index(\n",
    "    np.nanargmax(lppd_over_samples_and_chains),\n",
    "    lppd_over_samples_and_chains.shape,\n",
    ")\n",
    "print(min_idx)\n",
    "# annotate with a red cross\n",
    "plt.scatter(min_idx[1]+0.5, min_idx[0]+0.5, marker='x', color='white')\n",
    "plt.xlabel('Number of Chains')\n",
    "plt.ylabel('Number of Samples (Non-Linear!)')\n",
    "plt.title(\n",
    "    (\n",
    "        'LPPD over Samples and'\n",
    "        ' Chains (Higher is better)'\n",
    "    )\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration of the Credible Interval (CI) is assessed by the coverage probability (CP) of the 95% CI. The CP is the proportion of the true values that fall within the 95% CI. The CP should be close to 95% for a well-calibrated model. If the CP is too low, the model is overconfident, and if the CP is too high, the model is underconfident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in [0.25, 0.5, 0.75, 0.9, 0.98]:\n",
    "    hpdi_preds = hpdi(preds[good_chains_pred_indices], q)\n",
    "    acc_hpdi = jnp.mean(\n",
    "        (hpdi_preds[0, :] <= Y_val.squeeze())\n",
    "        & (hpdi_preds[1, :] >= Y_val.squeeze())\n",
    "    )\n",
    "    print(f'Accuracy of {int(q*100)}% HPDI: {acc_hpdi:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "# colors should be a 5 step gradient from orange to dark green\n",
    "colors = sns.color_palette('YlGn', 5)\n",
    "quantiles =  np.linspace(0.1, 0.9, 9)\n",
    "quantiles = np.concatenate([np.array([0.01, 0.05]), quantiles, np.array([0.95, 0.99])])\n",
    "# reverse the colors\n",
    "colors = colors[::-1]\n",
    "for trunc in [1, 10, 100, 1000, config[\"n_samples\"]]: # np.arange(100, config['n_samples'], 200):\n",
    "    nominal_coverage_vals = []\n",
    "    for q in quantiles:\n",
    "        hpdi_preds = hpdi(preds_chain_dim[good_chains, :, :][:, :trunc, :].reshape(-1, preds.shape[1]), q)\n",
    "        nominal_coverage_vals.append(jnp.mean(\n",
    "            (hpdi_preds[0, :] <= Y_val.squeeze())\n",
    "            & (hpdi_preds[1, :] >= Y_val.squeeze())\n",
    "        ))\n",
    "    # lineplot with nominal coverage on the y axis and the level of the HPDI on the x axis\n",
    "    plt.plot(\n",
    "        quantiles, \n",
    "        nominal_coverage_vals, \n",
    "        label=f'{trunc} samples',\n",
    "        color=colors.pop(),\n",
    "        # also points, the marker in black\n",
    "        marker='o',\n",
    "        markerfacecolor='black',\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel('Nominal Coverage Level')\n",
    "plt.ylabel('Observed Coverage Level')\n",
    "plt.title('Nominal Coverage of PP Credibility Intervals')\n",
    "# make the axis range from 0 to 1 and make it square \n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "# add a diagonal line\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "# shade the area between the diagonal line and the x axis and annotate with overconfidence\n",
    "plt.fill_between([0, 1], [0, 0], [0, 1], color='red', alpha=0.2)\n",
    "plt.annotate(\n",
    "    'Overconfidence',\n",
    "    xy=(0.75, 0.25),\n",
    "    xytext=(0.75, 0.25),\n",
    "    ha='center',\n",
    "    va = 'center',\n",
    "    color='red',\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "# colors should be a 5 step gradient from orange to dark green\n",
    "colors = sns.color_palette('YlGn', 5)\n",
    "quantiles =  np.array([0.01, 0.025, 0.05, 0.1])\n",
    "# reverse the colors\n",
    "colors = colors[::-1]\n",
    "for trunc in [1, 10, 100, 1000, config[\"n_samples\"]]: # np.arange(100, config['n_samples'], 200):\n",
    "    nominal_coverage_vals = []\n",
    "    for q in quantiles:\n",
    "        hpdi_preds = hpdi(preds_chain_dim[good_chains, :, :][:, :trunc, :].reshape(-1, preds.shape[1]), q)\n",
    "        nominal_coverage_vals.append(jnp.mean(\n",
    "            (hpdi_preds[0, :] <= Y_val.squeeze())\n",
    "            & (hpdi_preds[1, :] >= Y_val.squeeze())\n",
    "        ))\n",
    "    # lineplot with nominal coverage on the y axis and the level of the HPDI on the x axis\n",
    "    plt.plot(\n",
    "        quantiles, \n",
    "        nominal_coverage_vals, \n",
    "        label=f'{trunc} samples',\n",
    "        color=colors.pop(),\n",
    "        # also points, the marker in black\n",
    "        marker='o',\n",
    "        markerfacecolor='black',\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel('Nominal Coverage Level')\n",
    "plt.ylabel('Observed Coverage Level')\n",
    "plt.title('Nominal Coverage of PP Credibility Intervals')\n",
    "# make the axis range from 0 to 1 and make it square \n",
    "plt.ylim([0, 0.11])\n",
    "plt.xlim([0, 0.11])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "# add a diagonal line\n",
    "plt.plot([0, 0.11], [0, 0.11], color='black', linestyle='--')\n",
    "# shade the area between the diagonal line and the x axis and annotate with overconfidence\n",
    "plt.fill_between([0, 0.11], [0, 0], [0, 0.11], color='red', alpha=0.2)\n",
    "# axis ticks\n",
    "plt.xticks(quantiles)\n",
    "plt.annotate(\n",
    "    'Overconfidence',\n",
    "    xy=(0.08, 0.03),\n",
    "    xytext=(0.08, 0.03),\n",
    "    ha='center',\n",
    "    va = 'center',\n",
    "    color='red',\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "# colors should be a 5 step gradient from orange to dark green\n",
    "colors = sns.color_palette('YlGn', 5)\n",
    "quantiles =  np.linspace(0.1, 0.9, 9)\n",
    "quantiles = np.concatenate([np.array([0.01, 0.05]), quantiles, np.array([0.95, 0.99])])\n",
    "# reverse the colors\n",
    "colors = colors[::-1]\n",
    "trunc = 4000\n",
    "for chains in [1, 2, 4, 8, 10]: # np.arange(100, config['n_samples'], 200):\n",
    "    nominal_coverage_vals = []\n",
    "    for q in quantiles:\n",
    "        hpdi_preds = hpdi(preds_chain_dim[good_chains, ...][:chains, :, :][:, :trunc, :].reshape(-1, preds.shape[1]), q)\n",
    "        nominal_coverage_vals.append(jnp.mean(\n",
    "            (hpdi_preds[0, :] <= Y_val.squeeze())\n",
    "            & (hpdi_preds[1, :] >= Y_val.squeeze())\n",
    "        ))\n",
    "    # lineplot with nominal coverage on the y axis and the level of the HPDI on the x axis\n",
    "    plt.plot(\n",
    "        quantiles, \n",
    "        nominal_coverage_vals, \n",
    "        label=f'{chains} Chains',\n",
    "        color=colors.pop(),\n",
    "        # also points, the marker in black\n",
    "        marker='o',\n",
    "        markerfacecolor='black',\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel('Nominal Coverage Level')\n",
    "plt.ylabel('Observed Coverage Level')\n",
    "plt.title(f'Nominal Coverage of PP Credibility Intervals ({trunc} Samples)')\n",
    "# make the axis range from 0 to 1 and make it square \n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "# add a diagonal line\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "# shade the area between the diagonal line and the x axis and annotate with overconfidence\n",
    "plt.fill_between([0, 1], [0, 0], [0, 1], color='red', alpha=0.2)\n",
    "plt.annotate(\n",
    "    'Overconfidence',\n",
    "    xy=(0.75, 0.25),\n",
    "    xytext=(0.75, 0.25),\n",
    "    ha='center',\n",
    "    va = 'center',\n",
    "    color='red',\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "# colors should be a 5 step gradient from orange to dark green\n",
    "colors = sns.color_palette('YlGn', 5)\n",
    "quantiles =  np.array([0.01, 0.025, 0.05, 0.1])\n",
    "# reverse the colors\n",
    "colors = colors[::-1]\n",
    "trunc = 4000\n",
    "for chains in [1, 2, 4, 8, 10]: # np.arange(100, config['n_samples'], 200):\n",
    "    nominal_coverage_vals = []\n",
    "    for q in quantiles:\n",
    "        hpdi_preds = hpdi(preds_chain_dim[good_chains, ...][:chains, :, :][:, :trunc, :].reshape(-1, preds.shape[1]), q)\n",
    "        nominal_coverage_vals.append(jnp.mean(\n",
    "            (hpdi_preds[0, :] <= Y_val.squeeze())\n",
    "            & (hpdi_preds[1, :] >= Y_val.squeeze())\n",
    "        ))\n",
    "    # lineplot with nominal coverage on the y axis and the level of the HPDI on the x axis\n",
    "    plt.plot(\n",
    "        quantiles, \n",
    "        nominal_coverage_vals, \n",
    "        label=f'{chains} Chains',\n",
    "        color=colors.pop(),\n",
    "        # also points, the marker in black\n",
    "        marker='o',\n",
    "        markerfacecolor='black',\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel('Nominal Coverage Level')\n",
    "plt.ylabel('Observed Coverage Level')\n",
    "plt.title(f'Nominal Coverage of PP Credibility Intervals ({trunc} Samples)')\n",
    "# make the axis range from 0 to 1 and make it square \n",
    "plt.ylim([0, 0.11])\n",
    "plt.xlim([0, 0.11])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "# add a diagonal line\n",
    "plt.plot([0, 0.11], [0, 0.11], color='black', linestyle='--')\n",
    "# shade the area between the diagonal line and the x axis and annotate with overconfidence\n",
    "plt.fill_between([0, 0.11], [0, 0], [0, 0.11], color='red', alpha=0.2)\n",
    "# axis ticks\n",
    "plt.xticks(quantiles)\n",
    "plt.annotate(\n",
    "    'Overconfidence',\n",
    "    xy=(0.08, 0.03),\n",
    "    xytext=(0.08, 0.03),\n",
    "    ha='center',\n",
    "    va = 'center',\n",
    "    color='red',\n",
    ")\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_posterior_samples = {\n",
    "    k: v[good_chains, :truncate_samples, ...] for k, v in posterior_samples.items()\n",
    "}\n",
    "interchain_means_normal = pp_interchain_means(\n",
    "    trunc_posterior_samples, model, X_val\n",
    ")\n",
    "fig, ax = visualize_pp_chain_means(interchain_means_normal, 100, show=False)\n",
    "ax.set_xticklabels([str(i) for i in good_chains])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_chains_posterior_samples = {\n",
    "    k: v[good_chains, ...] for k, v in posterior_samples.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {}\n",
    "for param in good_chains_posterior_samples.keys():\n",
    "    all_params[param] = calculate_diagnostics(\n",
    "        good_chains_posterior_samples,\n",
    "        param,\n",
    "        truncate_samples,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list = list(all_params.keys())\n",
    "weight_parameters = [p for p in parameter_list if 'W' in p]\n",
    "bias_parameters = [p for p in parameter_list if 'b' in p]\n",
    "\n",
    "# calculate the average ESS for parameter layer\n",
    "ess_per_layer = {}\n",
    "bias_ess_per_layer = {}\n",
    "sd_ess_per_layer = {}\n",
    "sd_bias_ess_per_layer = {}\n",
    "for layer in range(1, len(weight_parameters)+1):\n",
    "    ess_per_layer[layer] = float(np.mean(all_params[f\"W{layer}\"][1]))\n",
    "    bias_ess_per_layer[layer] = float(np.mean(all_params[f\"b{layer}\"][1]))\n",
    "    # also the standard deviation of the ESS\n",
    "    sd_ess_per_layer[layer] = float(np.std(all_params[f\"W{layer}\"][1]))\n",
    "    sd_bias_ess_per_layer[layer] = float(np.std(all_params[f\"b{layer}\"][1]))\n",
    "# lineplot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(ess_per_layer.keys()),\n",
    "    y=list(ess_per_layer.values()),\n",
    "    ax=ax,\n",
    "    label='Weight',\n",
    "    color='blue',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(bias_ess_per_layer.keys()),\n",
    "    y=list(bias_ess_per_layer.values()),\n",
    "    ax=ax,\n",
    "    label='Bias',\n",
    "    color='orange',\n",
    ")\n",
    "# add the standard deviation of the ESS as error bars\n",
    "ax.errorbar(\n",
    "    list(ess_per_layer.keys()),\n",
    "    list(ess_per_layer.values()),\n",
    "    yerr=list(sd_ess_per_layer.values()),\n",
    "    color='blue',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(bias_ess_per_layer.keys()),\n",
    "    list(bias_ess_per_layer.values()),\n",
    "    yerr=list(sd_bias_ess_per_layer.values()),\n",
    "    color='orange',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Parameter Layer')\n",
    "ax.set_ylabel('Average ESS')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flatten all all_params[parameter][0] dicts in one dict\n",
    "all_params_rhat_stats = {}\n",
    "for param in all_params.keys():\n",
    "    paramdict = all_params[param][0]\n",
    "    for key in paramdict.keys():\n",
    "        all_params_rhat_stats[key] = np.concatenate(\n",
    "            [all_params_rhat_stats[key] , paramdict[key].reshape(-1)]\n",
    "        ) if key in all_params_rhat_stats.keys() else paramdict[key].reshape(-1)\n",
    "visualize_rhat(all_params_rhat_stats)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerwise_params_rhat_stats_weights = {}\n",
    "layerwise_params_rhat_stats_biases = {}\n",
    "for layer in range(1, len(weight_parameters)+1):\n",
    "    layerwise_params_rhat_stats_weights[layer] = {}\n",
    "    layerwise_params_rhat_stats_biases[layer] = {}\n",
    "    for param in [p for p in list(all_params.keys()) if (str(layer) in p) and ('W' in p)]:\n",
    "        paramdict = all_params[param][0]\n",
    "        for key in paramdict.keys():\n",
    "            layerwise_params_rhat_stats_weights[layer][key] = np.concatenate(\n",
    "                [layerwise_params_rhat_stats_weights[layer][key] , paramdict[key].reshape(-1)]\n",
    "            ) if key in layerwise_params_rhat_stats_weights[layer].keys() else paramdict[key].reshape(-1)\n",
    "    print(f\"Layer {layer} - Weights\")\n",
    "    display(visualize_rhat(layerwise_params_rhat_stats_weights[layer]))\n",
    "    for param in [p for p in list(all_params.keys()) if (str(layer) in p) and ('b' in p)]:\n",
    "        paramdict = all_params[param][0]\n",
    "        for key in paramdict.keys():\n",
    "            layerwise_params_rhat_stats_biases[layer][key] = np.concatenate(\n",
    "                [layerwise_params_rhat_stats_biases[layer][key] , paramdict[key].reshape(-1)]\n",
    "            ) if key in layerwise_params_rhat_stats_biases[layer].keys() else paramdict[key].reshape(-1)\n",
    "    print(f\"Layer {layer} - Biases\")\n",
    "    display(visualize_rhat(layerwise_params_rhat_stats_biases[layer]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerwise_rhat_w = [\n",
    "    jnp.mean(layerwise_params_rhat_stats_weights[layer][\"rhat\"]).item() for layer in layerwise_params_rhat_stats_weights.keys()\n",
    "]\n",
    "layerwise_rhat_sd_w = [\n",
    "    jnp.std(layerwise_params_rhat_stats_weights[layer][\"rhat\"]).item() for layer in layerwise_params_rhat_stats_weights.keys()\n",
    "]\n",
    "layerwise_split_rhat_w = [\n",
    "    jnp.mean(layerwise_params_rhat_stats_weights[layer][\"split_chain_rhat\"]).item() for layer in layerwise_params_rhat_stats_weights.keys()\n",
    "]\n",
    "layerwise_split_rhat_sd_w = [\n",
    "    jnp.std(layerwise_params_rhat_stats_weights[layer][\"split_chain_rhat\"]).item() for layer in layerwise_params_rhat_stats_weights.keys()\n",
    "]\n",
    "layerwise_rhat_b = [\n",
    "    jnp.mean(layerwise_params_rhat_stats_biases[layer][\"rhat\"]).item() for layer in layerwise_params_rhat_stats_biases.keys()\n",
    "]\n",
    "layerwise_rhat_sd_b = [\n",
    "    jnp.std(layerwise_params_rhat_stats_biases[layer][\"rhat\"]).item() for layer in layerwise_params_rhat_stats_biases.keys()\n",
    "]\n",
    "layerwise_split_rhat_b = [\n",
    "    jnp.mean(layerwise_params_rhat_stats_biases[layer][\"split_chain_rhat\"]).item() for layer in layerwise_params_rhat_stats_biases.keys()\n",
    "]\n",
    "layerwise_split_rhat_sd_b = [\n",
    "    jnp.std(layerwise_params_rhat_stats_biases[layer][\"split_chain_rhat\"]).item() for layer in layerwise_params_rhat_stats_biases.keys()\n",
    "]\n",
    "# visualize the rhat and split chain rhat for the weights and biases\n",
    "# use a different linetype for the weights and biases\n",
    "# use colors for rhat and split chain rhat\n",
    "# add a horizontal line at 1.1 and 1.01 and start the y axis at 1\n",
    "# also add errorbars for the standard deviation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(layerwise_params_rhat_stats_weights.keys()),\n",
    "    y=layerwise_rhat_w,\n",
    "    ax=ax,\n",
    "    label='$\\widehat{R}$ of Weights',\n",
    "    color='#06238f',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(layerwise_params_rhat_stats_biases.keys()),\n",
    "    y=layerwise_rhat_b,\n",
    "    ax=ax,\n",
    "    label='$\\widehat{R}$ of Biases',\n",
    "    color='#2e59f2',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(layerwise_params_rhat_stats_weights.keys()),\n",
    "    y=layerwise_split_rhat_w,\n",
    "    ax=ax,\n",
    "    label='Chainwise $\\widehat{R}$ of Weights',\n",
    "    color='#035c0c',\n",
    "    linestyle='--',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(layerwise_params_rhat_stats_biases.keys()),\n",
    "    y=layerwise_split_rhat_b,\n",
    "    ax=ax,\n",
    "    label='Chainwise $\\widehat{R}$ of Biases',\n",
    "    color='#26bf36',\n",
    "    linestyle='--',\n",
    ")\n",
    "# add the standard deviation of the ESS as error bars\n",
    "ax.errorbar(\n",
    "    list(layerwise_params_rhat_stats_weights.keys()),\n",
    "    layerwise_rhat_w,\n",
    "    yerr=layerwise_rhat_sd_w,\n",
    "    color='#06238f',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(layerwise_params_rhat_stats_biases.keys()),\n",
    "    layerwise_rhat_b,\n",
    "    yerr=layerwise_rhat_sd_b,\n",
    "    color='#2e59f2',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(layerwise_params_rhat_stats_weights.keys()),\n",
    "    layerwise_split_rhat_w,\n",
    "    yerr=layerwise_split_rhat_sd_w,\n",
    "    color='#035c0c',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(layerwise_params_rhat_stats_biases.keys()),\n",
    "    layerwise_split_rhat_b,\n",
    "    yerr=layerwise_split_rhat_sd_b,\n",
    "    color='#26bf36',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=1)\n",
    "ax.set_xlabel('Hidden Layer')\n",
    "ax.set_ylabel('')\n",
    "plt.close(fig)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the decomposition of the classic Rhat across layers (parameters) for both weights and biases (only use good chains)\n",
    "between_chain_var_w = {}\n",
    "within_chain_var_w = {}\n",
    "between_chain_var_b = {}\n",
    "within_chain_var_b = {}\n",
    "\n",
    "for param in good_chains_posterior_samples.keys():\n",
    "    if 'W' in param:\n",
    "        between_chain_var_w[param] = good_chains_posterior_samples[param].mean(axis = 1).std(axis = 0)**2\n",
    "        within_chain_var_w[param] = (good_chains_posterior_samples[param].std(axis = 1)**2).mean(axis = 0)\n",
    "    elif 'b' in param:\n",
    "        between_chain_var_b[param] = good_chains_posterior_samples[param].mean(axis = 1).std(axis = 0)\n",
    "        within_chain_var_b[param] = good_chains_posterior_samples[param].std(axis = 1).mean(axis = 0)\n",
    "# now compute the mean and standard deviation of the absolute values of the weights and biases\n",
    "between_chain_var_w_mean = {}\n",
    "between_chain_var_w_std = {}\n",
    "within_chain_var_w_mean = {}\n",
    "within_chain_var_w_std = {}\n",
    "between_chain_var_b_mean = {}\n",
    "between_chain_var_b_std = {}\n",
    "within_chain_var_b_mean = {}\n",
    "within_chain_var_b_std = {}\n",
    "for layer in range(1, len(weight_parameters)+1):\n",
    "    between_chain_var_w_mean[layer] = np.mean(between_chain_var_w[f\"W{layer}\"]).item()\n",
    "    between_chain_var_w_std[layer] = np.std(between_chain_var_w[f\"W{layer}\"]).item()\n",
    "    within_chain_var_w_mean[layer] = np.mean(within_chain_var_w[f\"W{layer}\"]).item()\n",
    "    within_chain_var_w_std[layer] = np.std(within_chain_var_w[f\"W{layer}\"]).item()\n",
    "    between_chain_var_b_mean[layer] = np.mean(between_chain_var_b[f\"b{layer}\"]).item()\n",
    "    between_chain_var_b_std[layer] = np.std(between_chain_var_b[f\"b{layer}\"]).item()\n",
    "    within_chain_var_b_mean[layer] = np.mean(within_chain_var_b[f\"b{layer}\"]).item()\n",
    "    within_chain_var_b_std[layer] = np.std(within_chain_var_b[f\"b{layer}\"]).item()\n",
    "# lineplot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(between_chain_var_w_mean.keys()),\n",
    "    y=list(between_chain_var_w_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Weight',\n",
    "    color='#06238f',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(between_chain_var_b_mean.keys()),\n",
    "    y=list(between_chain_var_b_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Bias',\n",
    "    color='#2e59f2',\n",
    ")\n",
    "# add the standard deviation of the ESS as error bars\n",
    "ax.errorbar(\n",
    "    list(between_chain_var_w_mean.keys()),\n",
    "    list(between_chain_var_w_mean.values()),\n",
    "    yerr=list(between_chain_var_w_std.values()),\n",
    "    color='#06238f',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(between_chain_var_b_mean.keys()),\n",
    "    list(between_chain_var_b_mean.values()),\n",
    "    yerr=list(between_chain_var_b_std.values()),\n",
    "    color='#2e59f2',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Avg. Between Chain Variance')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.close(fig)\n",
    "display(fig)\n",
    "\n",
    "# now the within chain variance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(within_chain_var_w_mean.keys()),\n",
    "    y=list(within_chain_var_w_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Weight',\n",
    "    color='#06238f',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(within_chain_var_b_mean.keys()),\n",
    "    y=list(within_chain_var_b_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Bias',\n",
    "    color='#2e59f2',\n",
    ")\n",
    "# add the standard deviation of the ESS as error bars\n",
    "ax.errorbar(\n",
    "    list(within_chain_var_w_mean.keys()),\n",
    "    list(within_chain_var_w_mean.values()),\n",
    "    yerr=list(within_chain_var_w_std.values()),\n",
    "    color='#06238f',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(within_chain_var_b_mean.keys()),\n",
    "    list(within_chain_var_b_mean.values()),\n",
    "    yerr=list(within_chain_var_b_std.values()),\n",
    "    color='#2e59f2',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Avg. Within Chain Variance')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.close(fig)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the absolut value of sampled weights and their standard deviation across layers (parameters) for both weights and biases (only use good chains)\n",
    "abs_weight_samples = {}\n",
    "abs_bias_samples = {}\n",
    "for param in good_chains_posterior_samples.keys():\n",
    "    if 'W' in param:\n",
    "        abs_weight_samples[param] = np.abs(good_chains_posterior_samples[param])\n",
    "    elif 'b' in param:\n",
    "        abs_bias_samples[param] = np.abs(good_chains_posterior_samples[param])\n",
    "# now compute the mean and standard deviation of the absolute values of the weights and biases\n",
    "abs_weight_samples_mean = {}\n",
    "abs_weight_samples_std = {}\n",
    "abs_bias_samples_mean = {}\n",
    "abs_bias_samples_std = {}\n",
    "for layer in range(1, len(weight_parameters)+1):\n",
    "    abs_weight_samples_mean[layer] = np.mean(abs_weight_samples[f\"W{layer}\"])\n",
    "    abs_weight_samples_std[layer] = np.std(abs_weight_samples[f\"W{layer}\"])\n",
    "    abs_bias_samples_mean[layer] = np.mean(abs_bias_samples[f\"b{layer}\"])\n",
    "    abs_bias_samples_std[layer] = np.std(abs_bias_samples[f\"b{layer}\"])\n",
    "# lineplot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=list(abs_weight_samples_mean.keys()),\n",
    "    y=list(abs_weight_samples_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Weight',\n",
    "    color='blue',\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=list(abs_bias_samples_mean.keys()),\n",
    "    y=list(abs_bias_samples_mean.values()),\n",
    "    ax=ax,\n",
    "    label='Bias',\n",
    "    color='orange',\n",
    ")\n",
    "# add the standard deviation of the ESS as error bars\n",
    "ax.errorbar(\n",
    "    list(abs_weight_samples_mean.keys()),\n",
    "    list(abs_weight_samples_mean.values()),\n",
    "    yerr=list(abs_weight_samples_std.values()),\n",
    "    color='blue',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    list(abs_bias_samples_mean.keys()),\n",
    "    list(abs_bias_samples_mean.values()),\n",
    "    yerr=list(abs_bias_samples_std.values()),\n",
    "    color='orange',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Average Absolute Value of Posterior (Parameter) Samples')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_split_chain_rhat = split_chain_r_hat(\n",
    "    preds_chain_dim[good_chains, :truncate_samples, :],\n",
    "    n_splits=4,\n",
    ")\n",
    "fig_pp_rhat = visualize_pp_rhat(pp_split_chain_rhat)\n",
    "fig_pp_rhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lppd_pointwise = model.get_lppd(X_val, Y_val, posterior_samples_raw, rolling=False)\n",
    "print(lppd_pointwise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lppd_pointwise_chain_dim = add_chain_dimension({'lppd': lppd_pointwise}, n_chains=exp_info[\"n_chains\"])['lppd']\n",
    "# only use the good chains\n",
    "lppd_pointwise_chain_dim = lppd_pointwise_chain_dim[good_chains, ...]\n",
    "ppd_pointwise_chain_dim = jnp.exp(lppd_pointwise_chain_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainwise_lpl = jnp.log(jnp.expand_dims(ppd_pointwise_chain_dim.mean(axis=2), axis=2))\n",
    "flat_lpl = chainwise_lpl[..., 0].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainwise_mean_lpl = chainwise_lpl[..., 0].mean(axis=1)\n",
    "chainwise_sd_lpl = chainwise_lpl[..., 0].std(axis=1)\n",
    "# visualize lineplot with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    x=good_chains,\n",
    "    y=chainwise_mean_lpl,\n",
    "    ax=ax,\n",
    "    color='blue',\n",
    ")\n",
    "ax.errorbar(\n",
    "    good_chains,\n",
    "    chainwise_mean_lpl,\n",
    "    yerr=chainwise_sd_lpl,\n",
    "    color='blue',\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_xlabel('Chain')\n",
    "ax.set_xticks(good_chains)\n",
    "ax.set_ylabel('Chainwise LPL')\n",
    "plt.close(fig)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize it as a colored traceplot\n",
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "chain_names = [f'chain_{i}' for i in range(len(good_chains))]\n",
    "colors = np.repeat(chain_names, truncate_samples, axis=0)\n",
    "sns.lineplot(\n",
    "    x=np.arange(len(flat_lpl)),\n",
    "    y=jnp.exp(flat_lpl),\n",
    "    ax=ax,\n",
    "    hue = colors\n",
    ")\n",
    "# ylim 0 1\n",
    "# ax.set_ylim([0, 1])\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('LPL')\n",
    "ax.set_title('LPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelman_split_r_hat(chainwise_lpl, n_splits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in a rolling window fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the chains\n",
    "chainwise_rolling_lppd = []\n",
    "for chain in good_chains:\n",
    "    chain_samples = {\n",
    "        k: v[chain, :truncate_samples, ...] for k, v in posterior_samples.items()\n",
    "    }\n",
    "    chainwise_rolling_lppd.append(\n",
    "        model.get_lppd(X_val, Y_val, chain_samples, rolling=True)\n",
    "    )\n",
    "chainwise_rolling_lppd = np.array(chainwise_rolling_lppd)\n",
    "chainwise_rolling_lppd = chainwise_rolling_lppd.mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_rolling_lppd = flatten_chain_dimension({\"clppd\": chainwise_rolling_lppd})[\"clppd\"]\n",
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "chain_names = [f'chain_{i}' for i in range(len(good_chains))]\n",
    "colors = np.repeat(chain_names, truncate_samples, axis=0)\n",
    "sns.lineplot(\n",
    "    x=np.array(list(np.arange(1, truncate_samples+1)) * len(good_chains)),\n",
    "    y=jnp.exp(flat_rolling_lppd),\n",
    "    ax=ax,\n",
    "    hue = colors\n",
    ")\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('PPD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the rmse instead of the lppd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_rmse(preds, Y):\n",
    "    return jnp.sqrt(jnp.mean((preds.squeeze() - Y.squeeze())**2))\n",
    "\n",
    "mse_per_chain = jnp.apply_along_axis(pointwise_rmse, 2, preds_chain_dim, Y_val)\n",
    "mse_per_chain = mse_per_chain[good_chains, ...]\n",
    "mse_flat = mse_per_chain.reshape(-1)\n",
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "chain_names = [f'chain_{i}' for i in range(len(good_chains))]\n",
    "colors = np.repeat(chain_names, truncate_samples, axis=0)\n",
    "sns.lineplot(\n",
    "    x=np.arange(len(mse_flat)),\n",
    "    y=(mse_flat),\n",
    "    ax=ax,\n",
    "    hue = colors\n",
    ")\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('RMSE of each Posterior Sample induced Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelman_split_r_hat(jnp.expand_dims(mse_per_chain[good_chains, ...], 2), n_splits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rmse for predictions with low and high and compare\n",
    "# to rmse with low rhat\n",
    "low_rhat = [i for i in range(preds_chain_dim.shape[2]) if pp_split_chain_rhat[\"rhat\"][i] < 1.1]\n",
    "print(f\"Number of predictions with low Rhat: {len(low_rhat)}\")\n",
    "rhat_low_rmse = np.sqrt(\n",
    "    mse(\n",
    "        preds_chain_dim[good_chains, :truncate_samples, :][:, :, \n",
    "            low_rhat\n",
    "        ],\n",
    "        Y_val[low_rhat, :],\n",
    "    )[0]\n",
    ")\n",
    "print(\"RMSE for predictions with low Rhat\")\n",
    "print(rhat_low_rmse)\n",
    "high_rhat = [i for i in range(preds_chain_dim.shape[2]) if pp_split_chain_rhat[\"rhat\"][i] >= 1.1]\n",
    "print(f\"Number of predictions with high Rhat: {len(high_rhat)}\")\n",
    "rhat_high_rmse = np.sqrt(\n",
    "    mse(\n",
    "        preds_chain_dim[good_chains, :truncate_samples, :][:, :, \n",
    "            high_rhat\n",
    "        ],\n",
    "        Y_val[high_rhat, :],\n",
    "    )[0]\n",
    ")\n",
    "print(\"RMSE for predictions with high Rhat\")\n",
    "print(rhat_high_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
