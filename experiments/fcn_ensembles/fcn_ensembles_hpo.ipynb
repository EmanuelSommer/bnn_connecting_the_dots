{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:39:16.266452890Z",
     "start_time": "2024-02-01T17:39:15.975057431Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 14\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      8\u001B[0m     Dict,\n\u001B[1;32m      9\u001B[0m     Final,\n\u001B[1;32m     10\u001B[0m     List,\n\u001B[1;32m     11\u001B[0m     Union,\n\u001B[1;32m     12\u001B[0m )\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpytorch_lightning\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpl\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "from typing import (\n",
    "    Dict,\n",
    "    Final,\n",
    "    List,\n",
    "    Union,\n",
    ")\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import probabilisticml as pml\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ENSEMBLE_SIZE: Final = 12\n",
    "MAX_EPOCHS: Final = 5000\n",
    "WEIGHT_DECAY: Final = [0.01, 0.001, 0.0001]\n",
    "BATCH_SIZE: Final = [32, 64]\n",
    "VAL_SIZE: Final = [0., 0.1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec6edec7a215789"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_config(config_path: str) -> tuple[dict, list]:\n",
    "    \"\"\"Load the configuration file.\"\"\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "def save_config(config, path):\n",
    "    \"\"\"Save the configuration file.\"\"\"\n",
    "    with open(path, 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "\n",
    "\n",
    "def load_data(dataset: str, seed: int, val_size: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Load the training data.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Name of the dataset to load.\n",
    "        seed (int): Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        X_train (jax.numpy.ndarray): Input features for training.\n",
    "        Y_train (jax.numpy.ndarray): Target values for training.\n",
    "    \"\"\"\n",
    "    regr_dataset = pml.data.dataset.DatasetTabular(\n",
    "        data_path=f'data/{dataset}',\n",
    "        target_indices=[],\n",
    "        split_spec={'train': 0.8 - val_size, 'val': val_size, 'test': 0.2},\n",
    "        seed=seed,\n",
    "        standardize=True,\n",
    "    )\n",
    "    X_train, Y_train = regr_dataset.get_data(split='train', data_type='jax')\n",
    "    X_val, Y_val = regr_dataset.get_data(split='val', data_type='jax')\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "\n",
    "def exp_tuple_to_dict(exp: tuple) -> dict:\n",
    "    \"\"\"Convert an experiment tuple to a dictionary.\"\"\"\n",
    "    return {\n",
    "        'data': exp[0],\n",
    "        'activation': exp[1],\n",
    "        'hidden_structure': exp[2],\n",
    "        'replications': exp[3],\n",
    "    }\n",
    "\n",
    "\n",
    "def experiment_generator(config: dict) -> dict:\n",
    "    \"\"\"Generate the experiments (Cartesian Product).\"\"\"\n",
    "    exp_dimensions = []\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, list):\n",
    "            exp_dimensions.append(value)\n",
    "        elif isinstance(value, dict):\n",
    "            exp_dimensions.append(list(value.keys()))\n",
    "        else:\n",
    "            exp_dimensions.append([value])\n",
    "\n",
    "    exp_tuples = list(itertools.product(*exp_dimensions))\n",
    "    experiments = {\n",
    "        f'exp{str(i)}|' + '|'.join([str(e) for e in p]): p\n",
    "        for i, p in enumerate(exp_tuples)\n",
    "    }\n",
    "    return experiments\n",
    "\n",
    "\n",
    "def init_weights(layer: nn.Module) -> None:\n",
    "    \"\"\"Create checkpoint with network(s) to be loaded in learning.\"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "        nn.init.zeros_(layer.bias)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41cc5d8c31fce88a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_sizes: List[int],\n",
    "        activation: nn.modules.activation,\n",
    "        dropout_ratio: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Instantiate MLP.\"\"\"\n",
    "        super().__init__()\n",
    "        hidden_id = '_'.join([str(x) for x in hidden_sizes])\n",
    "        self.model_id = f'MLP_{input_size}_{hidden_id}_2'\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.net = torch.nn.Sequential(torch.nn.Linear(input_size, hidden_sizes[0]))\n",
    "        for i, o in zip(hidden_sizes, hidden_sizes[1:] + [2]):\n",
    "            self.net.append(activation())\n",
    "            self.net.append(torch.nn.Linear(i, o))\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Define forward pass.\"\"\"\n",
    "        x = self.net(x)\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da071d4ea07846f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NNLearner(pl.LightningModule):\n",
    "    \"\"\"Vanilla network training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_train: torch.Tensor,\n",
    "        y_train: torch.Tensor,\n",
    "        x_val: torch.Tensor,\n",
    "        y_val: torch.Tensor,\n",
    "        model: nn.Module,\n",
    "        training_specs: Dict,\n",
    "        seed: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Set up learner object.\"\"\"\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "        self.model = model\n",
    "        self.data_train = TensorDataset(x_train, y_train)\n",
    "        self.data_val = TensorDataset(x_val, y_val)\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.model.parameters(), weight_decay=training_specs['weight_decay']\n",
    "        )\n",
    "        self.scheduler = None\n",
    "        self.training_specs = training_specs\n",
    "\n",
    "    def on_fit_start(self) -> None:\n",
    "        \"\"\"Set global seed.\"\"\"\n",
    "        pl.seed_everything(seed=self.seed)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Set up data loader for training.\"\"\"\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.training_specs.get('batch_size'),\n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> Union[DataLoader, None]:\n",
    "        \"\"\"Set up data loader for validation.\"\"\"\n",
    "        if len(self.data_val.tensors[0]) == 0:\n",
    "            return self.train_dataloader()\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.data_val,\n",
    "                batch_size=self.data_val.tensors[0].shape[0],\n",
    "                num_workers=1,\n",
    "            )\n",
    "\n",
    "    def configure_optimizers(self) -> Dict:\n",
    "        \"\"\"Set up optimization-related objects.\"\"\"\n",
    "        return {'optimizer': self.optimizer}\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Define standard forward pass.\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch: Dict, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Define training routine.\"\"\"\n",
    "        x, y = batch\n",
    "        outputs = self.model(x)\n",
    "        mean_pred = outputs[:, 0]\n",
    "        std_pred = torch.exp(outputs[:, 1])\n",
    "        loss = torch.nn.functional.gaussian_nll_loss(mean_pred, y.squeeze(), std_pred)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict, batch_idx: int) -> None:\n",
    "        \"\"\"Define validation routine.\"\"\"\n",
    "        if len(self.data_val.tensors[1]) > 0:\n",
    "            x, y = batch\n",
    "            outputs = self.model(x)\n",
    "            mean_pred = outputs[:, 0]\n",
    "            std_pred = torch.exp(outputs[:, 1])\n",
    "            loss_val = torch.nn.functional.gaussian_nll_loss(\n",
    "                mean_pred, y.squeeze(), std_pred\n",
    "            )\n",
    "            self.log('loss_val', loss_val)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "994d8e6bcf3dc7b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SGDEnsemble:\n",
    "    \"\"\"Ensemble of SGD trained models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_learner: nn.Module, ensemble_size: int, ckpt: str = ''\n",
    "    ) -> None:\n",
    "        \"\"\"Instantiate ensemble.\"\"\"\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.base_learner = base_learner\n",
    "        self.ckpt = ckpt\n",
    "        self.weights = []\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        x: torch.tensor,\n",
    "        y: torch.tensor,\n",
    "        x_val: torch.tensor,\n",
    "        y_val: torch.tensor,\n",
    "        log_at_epoch: list,\n",
    "        training_specs: dict,\n",
    "    ) -> None:\n",
    "        \"\"\"Train the ensemble.\"\"\"\n",
    "        if len(self.ckpt) == 0 and len(log_at_epoch) > 0:\n",
    "            raise ValueError('Logging requires path to checkpoint')\n",
    "\n",
    "        for idx in range(self.ensemble_size):\n",
    "            bl = copy.deepcopy(self.base_learner)\n",
    "            bl.apply(init_weights)\n",
    "            nn_learner = NNLearner(\n",
    "                x_train=x,\n",
    "                y_train=y,\n",
    "                x_val=x_val,\n",
    "                y_val=y_val,\n",
    "                model=bl,\n",
    "                training_specs=training_specs,\n",
    "                seed=idx,\n",
    "            )\n",
    "            if len(y_val) > 0:\n",
    "                callback = [EarlyStopping('loss_val', patience=30, check_finite=False)]\n",
    "            else:\n",
    "                callback = []\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=training_specs['max_epochs'],\n",
    "                num_sanity_val_steps=0,\n",
    "                deterministic=True,\n",
    "                callbacks=callback,\n",
    "            )\n",
    "            print(f'---> Training ensemble member {idx + 1}...')\n",
    "            trainer.fit(nn_learner)\n",
    "\n",
    "            stdict = nn_learner.model.state_dict()\n",
    "            weight_keys_in = list(stdict.keys())\n",
    "            weight_keys_out = []\n",
    "            for i in range(len(weight_keys_in) // 2):\n",
    "                weight_keys_out.append(f'W{i + 1}')\n",
    "                weight_keys_out.append(f'b{i + 1}')\n",
    "            final_weights = {}\n",
    "            for i, o in zip(weight_keys_in, weight_keys_out):\n",
    "                final_weights[o] = bl.state_dict()[i].data.numpy()\n",
    "            np.savez(os.path.join(self.ckpt, f'{idx}.npz'), **final_weights)\n",
    "            torch.save(stdict, os.path.join(self.ckpt, f'stdict_{idx}.pt'))\n",
    "\n",
    "    def predict(self, x: torch.tensor):\n",
    "        \"\"\"Predict with the ensemble.\"\"\"\n",
    "        ensemble_prediction = []\n",
    "        for idx in range(self.ensemble_size):\n",
    "            bl = self.base_learner\n",
    "            bl.load_state_dict(self.weights[idx])\n",
    "            prediction = bl(x)\n",
    "            ensemble_prediction.append(prediction)\n",
    "        return torch.stack(tuple(ensemble_prediction))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57c327c21acc529b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    exp: tuple, path: str, training_specs: dict, val_size: float\n",
    ") -> None:\n",
    "    \"\"\"Run a single experiment.\"\"\"\n",
    "    start_time = time.time()\n",
    "    # load the data\n",
    "    X_train, Y_train, X_val, Y_val = load_data(exp[0], seed=exp[3], val_size=val_size)\n",
    "    exp_dict = exp_tuple_to_dict(exp)\n",
    "    if training_specs['batch_size'] == -1:\n",
    "        training_specs['batch_size'] = X_train.shape[0]\n",
    "\n",
    "    if exp_dict['activation'] == 'relu':\n",
    "        activation = nn.ReLU\n",
    "    elif exp_dict['activation'] == 'tanh':\n",
    "        activation = nn.Tanh\n",
    "    else:\n",
    "        raise ValueError(f'Activation {exp_dict[\"activation\"]} not supported.')\n",
    "\n",
    "    # initialize the model\n",
    "    base_learner = MLP(\n",
    "        input_size=X_train.shape[1],\n",
    "        hidden_sizes=[int(d) for d in exp_dict['hidden_structure'].split('-')],\n",
    "        activation=activation,\n",
    "        dropout_ratio=0.0,\n",
    "    )\n",
    "    deep_ensemble = SGDEnsemble(\n",
    "        base_learner=base_learner,\n",
    "        ensemble_size=ENSEMBLE_SIZE,\n",
    "        ckpt=path,\n",
    "    )\n",
    "    deep_ensemble.train(\n",
    "        x=torch.from_numpy(np.array(X_train)),\n",
    "        y=torch.from_numpy(np.array(Y_train)),\n",
    "        x_val=torch.from_numpy(np.array(X_val)),\n",
    "        y_val=torch.from_numpy(np.array(Y_val)),\n",
    "        log_at_epoch=[],\n",
    "        training_specs=training_specs,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'{(time.time() - start_time) / 60:.2f} min '\n",
    "        f'for {training_specs[\"exp_identifier\"]}'\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d329a300a110e1da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main_path = 'results/de/'\n",
    "os.makedirs(main_path, exist_ok=True)\n",
    "config = load_config('experiments/fcn_ensembles/config.yaml')\n",
    "experiments = experiment_generator(config)\n",
    "training_configs = itertools.product(WEIGHT_DECAY, BATCH_SIZE, VAL_SIZE)\n",
    "\n",
    "# Run the experiments\n",
    "for exp_name, exp in experiments.items():\n",
    "    for wd, bs, vs in training_configs:\n",
    "        expd = exp_tuple_to_dict(exp)\n",
    "        isval = 'val' if vs > 0 else 'noval'\n",
    "        exp_identifier = (\n",
    "            f'{expd[\"data\"]}|{expd[\"hidden_structure\"]}|{expd[\"activation\"]}|'\n",
    "            + f'wd{str(wd)}|bs{str(bs)}|{isval}|{expd[\"replications\"]}|'\n",
    "        )\n",
    "        training_specs = {\n",
    "            'max_epochs': MAX_EPOCHS,\n",
    "            'weight_decay': wd,\n",
    "            'batch_size': bs,\n",
    "            'exp_identifier': exp_identifier,\n",
    "        }\n",
    "        dir_name = os.path.join(main_path, exp_identifier)\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "        run_experiment(exp, dir_name, training_specs, vs)\n",
    "\n",
    "    print('All experiments have been run.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6f7dca2d6d09903"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
