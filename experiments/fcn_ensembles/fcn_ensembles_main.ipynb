{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "from typing import Final, List\n",
    "import numpy as np\n",
    "import probabilisticml as pml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ENSEMBLE_SIZE: Final = 12\n",
    "NUM_EPOCHS: Final = 5000"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b13c2e5d3675f6c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_config(config_path: str) -> tuple[dict, list]:\n",
    "    \"\"\"Load the configuration file.\"\"\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "def save_config(config, path):\n",
    "    \"\"\"Save the configuration file.\"\"\"\n",
    "    with open(path, 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "\n",
    "\n",
    "def load_data(dataset: str, seed: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Load the training data.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Name of the dataset to load.\n",
    "        seed (int): Seed for the random number generator.\n",
    "    Returns:\n",
    "        X_train (jax.numpy.ndarray): Input features for training.\n",
    "        Y_train (jax.numpy.ndarray): Target values for training.\n",
    "    \"\"\"\n",
    "    regr_dataset = pml.data.dataset.DatasetTabular(\n",
    "        data_path=f'../../data/{dataset}',\n",
    "        target_indices=[],\n",
    "        split_spec={'train': 0.8, 'test': 0.2},\n",
    "        seed=seed,\n",
    "        standardize=True,\n",
    "    )\n",
    "    X_train, Y_train = regr_dataset.get_data(split='train', data_type='jax')\n",
    "\n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "def exp_tuple_to_dict(exp: tuple) -> dict:\n",
    "    \"\"\"Convert an experiment tuple to a dictionary.\"\"\"\n",
    "    return {\n",
    "        'data': exp[0],\n",
    "        'activation': exp[1],\n",
    "        'hidden_structure': [int(d) for d in exp[2].split('-')],\n",
    "        'replications': exp[3],\n",
    "    }\n",
    "\n",
    "\n",
    "def experiment_generator(config: dict) -> dict:\n",
    "    \"\"\"Generate the experiments (Cartesian Product).\"\"\"\n",
    "    exp_dimensions = []\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, list):\n",
    "            exp_dimensions.append(value)\n",
    "        elif isinstance(value, dict):\n",
    "            exp_dimensions.append(list(value.keys()))\n",
    "        else:\n",
    "            exp_dimensions.append([value])\n",
    "\n",
    "    exp_tuples = list(itertools.product(*exp_dimensions))\n",
    "    experiments = {\n",
    "        f'exp{str(i)}|' + '|'.join([str(e) for e in p]): p\n",
    "        for i, p in enumerate(exp_tuples)\n",
    "    }\n",
    "    return experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "313ed45d59324b2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_weights(layer: nn.Module) -> None:\n",
    "    \"\"\"Create checkpoint with network(s) to be loaded in learning.\"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "        nn.init.zeros_(layer.bias)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_sizes: List[int],\n",
    "        activation: nn.modules.activation,\n",
    "        dropout_ratio: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Instantiate MLP.\"\"\"\n",
    "        super().__init__()\n",
    "        hidden_id = '_'.join([str(x) for x in hidden_sizes])\n",
    "        self.model_id = f'MLP_{input_size}_{hidden_id}_2'\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.net = torch.nn.Sequential(torch.nn.Linear(input_size, hidden_sizes[0]))\n",
    "        for i, o in zip(hidden_sizes, hidden_sizes[1:] + [2]):\n",
    "            self.net.append(activation())\n",
    "            self.net.append(torch.nn.Linear(i, o))\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Define forward pass.\"\"\"\n",
    "        x = self.net(x)\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e78309019fa0925"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SGDEnsemble:\n",
    "    \"\"\"Ensemble of SGD trained models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_learner: nn.Module, ensemble_size: int, ckpt: str = ''\n",
    "    ) -> None:\n",
    "        \"\"\"Instantiate ensemble.\"\"\"\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.base_learner = base_learner\n",
    "        self.ckpt = ckpt\n",
    "        self.weights = []\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_epochs: int,\n",
    "        x: torch.tensor,\n",
    "        y: torch.tensor,\n",
    "        criterion: torch.nn.modules.loss,\n",
    "        log_at_epoch: list,\n",
    "    ) -> None:\n",
    "        \"\"\"Train the ensemble.\"\"\"\n",
    "        if len(self.ckpt) == 0 and len(log_at_epoch) > 0:\n",
    "            raise ValueError('Logging requires path to checkpoint')\n",
    "\n",
    "        for idx in range(self.ensemble_size):\n",
    "            bl = copy.deepcopy(self.base_learner)\n",
    "            torch.manual_seed(idx)\n",
    "            bl.apply(init_weights)\n",
    "            opt = optim.Adam(bl.parameters(), weight_decay=0.01)\n",
    "            with tqdm(total=num_epochs, desc='Training Progress') as pbar:\n",
    "                for epoch in range(num_epochs):\n",
    "                    # Forward pass\n",
    "                    outputs = bl(x)\n",
    "                    mean_pred = outputs[:, 0]\n",
    "                    std_pred = torch.exp(outputs[:, 1])\n",
    "                    loss = criterion(mean_pred, y.squeeze(), std_pred)\n",
    "\n",
    "                    if (\n",
    "                        torch.isnan(loss).any()\n",
    "                        or torch.isinf(loss).any()\n",
    "                        or loss.item() < -1e6\n",
    "                    ):\n",
    "                        print('Loss exploded, breaking')\n",
    "                        break\n",
    "                    # Backward pass and optimization\n",
    "                    opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix_str('Loss: {:.4f}'.format(loss.item()))\n",
    "\n",
    "                # Weights\n",
    "                weight_keys_in = list(bl.state_dict().keys())\n",
    "                weight_keys_out = []\n",
    "                for i in range(len(weight_keys_in) // 2):\n",
    "                    weight_keys_out.append(f'W{i + 1}')\n",
    "                    weight_keys_out.append(f'b{i + 1}')\n",
    "                final_weights = {}\n",
    "                for i, o in zip(weight_keys_in, weight_keys_out):\n",
    "                    final_weights[o] = bl.state_dict()[i].data.numpy()\n",
    "                np.savez(os.path.join(self.ckpt, f'{idx}.npz'), **final_weights)\n",
    "                torch.save(bl.state_dict(), os.path.join(self.ckpt, f'stdict_{idx}.pt'))\n",
    "\n",
    "    def predict(self, x: torch.tensor):\n",
    "        \"\"\"Predict with the ensemble.\"\"\"\n",
    "        ensemble_prediction = []\n",
    "        for idx in range(self.ensemble_size):\n",
    "            bl = self.base_learner\n",
    "            bl.load_state_dict(self.weights[idx])\n",
    "            prediction = bl(x)\n",
    "            ensemble_prediction.append(prediction)\n",
    "        return torch.stack(tuple(ensemble_prediction))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe5815b242b96522"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_experiment(exp_name: str, exp: tuple, path: str) -> None:\n",
    "    \"\"\"Run a single experiment.\"\"\"\n",
    "    start_time = time.time()\n",
    "    # load the data\n",
    "    X_train, Y_train = load_data(exp[0], seed=exp[3])\n",
    "    exp_dict = exp_tuple_to_dict(exp)\n",
    "\n",
    "    if exp_dict['activation'] == 'relu':\n",
    "        activation = nn.ReLU\n",
    "    elif exp_dict['activation'] == 'tanh':\n",
    "        activation = nn.Tanh\n",
    "    else:\n",
    "        raise ValueError(f'Activation {exp_dict[\"activation\"]} not supported.')\n",
    "\n",
    "    # initialize the model\n",
    "    base_learner = MLP(\n",
    "        input_size=X_train.shape[1],\n",
    "        hidden_sizes=[16, 16],\n",
    "        activation=activation,\n",
    "        dropout_ratio=0.0,\n",
    "    )\n",
    "    deep_ensemble = SGDEnsemble(\n",
    "        base_learner=base_learner,\n",
    "        ensemble_size=ENSEMBLE_SIZE,\n",
    "        ckpt=path,\n",
    "    )\n",
    "    deep_ensemble.train(\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        x=torch.from_numpy(np.array(X_train)),\n",
    "        y=torch.from_numpy(np.array(Y_train)),\n",
    "        criterion=nn.GaussianNLLLoss(),\n",
    "        log_at_epoch=[],\n",
    "    )\n",
    "\n",
    "    print(f'{(time.time() - start_time) / 60:.2f} min for {exp_name}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efc4076bb24876e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main_path = '../../results/de/'\n",
    "os.makedirs(main_path, exist_ok=True)\n",
    "config = load_config('../../experiments/fcn_ensembles/config.yaml')\n",
    "experiments = experiment_generator(config)\n",
    "\n",
    "# Run the experiments\n",
    "for exp_name, exp in experiments.items():\n",
    "    expd = exp_tuple_to_dict(exp)\n",
    "    exp_identifier = (\n",
    "        f'{expd[\"data\"]}|{expd[\"hidden_structure\"]}|' + f'{expd[\"activation\"]}|'\n",
    "    )\n",
    "    dir_name = os.path.join(main_path, exp_identifier)\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    run_experiment(exp, exp, dir_name)\n",
    "\n",
    "print('All experiments have been run.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "854f6bc4e26c0f47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
